{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "da5cb3b1c6c140da96f367f4493734e9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_98cac349d45444faa5db226cbe82351c",
              "IPY_MODEL_4022ad525b0c4faab0a8305e0fe2e6ee",
              "IPY_MODEL_43f3ce7eed8e4e9a9fd5b54eda9a2c62"
            ],
            "layout": "IPY_MODEL_058cd07698cd4e04a500c2d60d3ec2fa"
          }
        },
        "98cac349d45444faa5db226cbe82351c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6d4944828fb84f4db262740f869dd904",
            "placeholder": "​",
            "style": "IPY_MODEL_a82d8e090a6f47b780eeb82b2de4b9e9",
            "value": "tokenizer_config.json: 100%"
          }
        },
        "4022ad525b0c4faab0a8305e0fe2e6ee": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5bef0647f64f4a6aa8b1129492f86666",
            "max": 26,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_08a7c4a3aff74157970b869624fc5108",
            "value": 26
          }
        },
        "43f3ce7eed8e4e9a9fd5b54eda9a2c62": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0ee20a580c594b2bac6686ce5700a87c",
            "placeholder": "​",
            "style": "IPY_MODEL_d0e41ee0cb3b437ebdf95419deaa17c8",
            "value": " 26.0/26.0 [00:00&lt;00:00, 714B/s]"
          }
        },
        "058cd07698cd4e04a500c2d60d3ec2fa": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6d4944828fb84f4db262740f869dd904": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a82d8e090a6f47b780eeb82b2de4b9e9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "5bef0647f64f4a6aa8b1129492f86666": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "08a7c4a3aff74157970b869624fc5108": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "0ee20a580c594b2bac6686ce5700a87c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d0e41ee0cb3b437ebdf95419deaa17c8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "6df92aceb9484ead8bddeb5b1c683107": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_02166dc287cc42f29a15119bb0369100",
              "IPY_MODEL_a3c108526ba74fd2b82ee9df8b87a12e",
              "IPY_MODEL_a25a5c419d76480b90472fb9bd7de2d8"
            ],
            "layout": "IPY_MODEL_a5b8d24f4fac46f0bf04a2972cd117f5"
          }
        },
        "02166dc287cc42f29a15119bb0369100": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8485fee50df74a8b9f182f337419fc92",
            "placeholder": "​",
            "style": "IPY_MODEL_6f59d88949844f5991114af6fea5f149",
            "value": "vocab.json: 100%"
          }
        },
        "a3c108526ba74fd2b82ee9df8b87a12e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6a1be352061b40c69924fbcb0fc673d1",
            "max": 898822,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_de183776f76d4780a0656218b48b28df",
            "value": 898822
          }
        },
        "a25a5c419d76480b90472fb9bd7de2d8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b3136ce438f74aeeab6bc11bf52db002",
            "placeholder": "​",
            "style": "IPY_MODEL_e373234271a94233b77ca407da0fa921",
            "value": " 899k/899k [00:00&lt;00:00, 2.08MB/s]"
          }
        },
        "a5b8d24f4fac46f0bf04a2972cd117f5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8485fee50df74a8b9f182f337419fc92": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6f59d88949844f5991114af6fea5f149": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "6a1be352061b40c69924fbcb0fc673d1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "de183776f76d4780a0656218b48b28df": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "b3136ce438f74aeeab6bc11bf52db002": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e373234271a94233b77ca407da0fa921": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "7458d5532f494d468e5853c314bd96ad": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_7ed6986f9e7c4f75ab27079e492e44fb",
              "IPY_MODEL_cbd3667fb68049f78c5d745896aaef65",
              "IPY_MODEL_5c272dbaa2a84337b38a4b4d2fdfddfb"
            ],
            "layout": "IPY_MODEL_5354473cf3d645668a81f783728b228d"
          }
        },
        "7ed6986f9e7c4f75ab27079e492e44fb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_92d4b121a4f848f385bb904dfb8c571c",
            "placeholder": "​",
            "style": "IPY_MODEL_ca5e30ae943a4c678d2d43b1f2245c2e",
            "value": "merges.txt: 100%"
          }
        },
        "cbd3667fb68049f78c5d745896aaef65": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2c79940bdece47a3bd5a3a3dbb07b329",
            "max": 456318,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_c2735f4281a642baac1b7b605dc428eb",
            "value": 456318
          }
        },
        "5c272dbaa2a84337b38a4b4d2fdfddfb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2930c178762340d3b9325970fc273c49",
            "placeholder": "​",
            "style": "IPY_MODEL_e46c2f25f2e94e2787f560c2d6307840",
            "value": " 456k/456k [00:00&lt;00:00, 5.75MB/s]"
          }
        },
        "5354473cf3d645668a81f783728b228d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "92d4b121a4f848f385bb904dfb8c571c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ca5e30ae943a4c678d2d43b1f2245c2e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "2c79940bdece47a3bd5a3a3dbb07b329": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c2735f4281a642baac1b7b605dc428eb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "2930c178762340d3b9325970fc273c49": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e46c2f25f2e94e2787f560c2d6307840": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "f4d33f579b59408381e3e46ace181dde": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_294c8b4c39be4beab5c9170a76467f56",
              "IPY_MODEL_faff09cb171a4bcc9ff3ad5c076a66d7",
              "IPY_MODEL_bcde1c7b60fa4b7fb51ea37bbdbb90a5"
            ],
            "layout": "IPY_MODEL_d896d50918ee4c8fa03c41bddb7ebfad"
          }
        },
        "294c8b4c39be4beab5c9170a76467f56": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1f548f90477046a990398de665a47f3c",
            "placeholder": "​",
            "style": "IPY_MODEL_d80e8f5d076b47b582415ab0952c016a",
            "value": "tokenizer.json: 100%"
          }
        },
        "faff09cb171a4bcc9ff3ad5c076a66d7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_62ca4b80f0714c1983d1a35e31a88403",
            "max": 1355863,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_0863dd793c9f4b3ebae3cbb02990261d",
            "value": 1355863
          }
        },
        "bcde1c7b60fa4b7fb51ea37bbdbb90a5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_10ef2edd55ba4ad9beabe87831103d92",
            "placeholder": "​",
            "style": "IPY_MODEL_dbccb8cd7bb7440d9aafa22fc52c6d21",
            "value": " 1.36M/1.36M [00:00&lt;00:00, 6.40MB/s]"
          }
        },
        "d896d50918ee4c8fa03c41bddb7ebfad": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1f548f90477046a990398de665a47f3c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d80e8f5d076b47b582415ab0952c016a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "62ca4b80f0714c1983d1a35e31a88403": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0863dd793c9f4b3ebae3cbb02990261d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "10ef2edd55ba4ad9beabe87831103d92": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "dbccb8cd7bb7440d9aafa22fc52c6d21": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "762eed4f46e845549166d1b51c22608c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_6d7d9548fe314daa9cb9ab089b8f5e52",
              "IPY_MODEL_c8c0219d5f06477f87a23e2d91c21fb8",
              "IPY_MODEL_340b87f179264d6d927633af5b0e2e67"
            ],
            "layout": "IPY_MODEL_772843cb165246b3abe661c61cf10fcf"
          }
        },
        "6d7d9548fe314daa9cb9ab089b8f5e52": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_85980c6e960241f7b8beee5a39772acd",
            "placeholder": "​",
            "style": "IPY_MODEL_9fa75e21194847c1a1337f937c892230",
            "value": "config.json: 100%"
          }
        },
        "c8c0219d5f06477f87a23e2d91c21fb8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2f8cdfce89994638b7befe7e6003c3e2",
            "max": 1628,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_55acdc60a2e746d3ada3e909d6afb780",
            "value": 1628
          }
        },
        "340b87f179264d6d927633af5b0e2e67": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6873bb548b1e4c0f88ccd903daaea7cd",
            "placeholder": "​",
            "style": "IPY_MODEL_1576511c15a24bfc869e4be59e5e0b48",
            "value": " 1.63k/1.63k [00:00&lt;00:00, 21.1kB/s]"
          }
        },
        "772843cb165246b3abe661c61cf10fcf": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "85980c6e960241f7b8beee5a39772acd": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9fa75e21194847c1a1337f937c892230": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "2f8cdfce89994638b7befe7e6003c3e2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "55acdc60a2e746d3ada3e909d6afb780": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "6873bb548b1e4c0f88ccd903daaea7cd": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1576511c15a24bfc869e4be59e5e0b48": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "c4a94a7092a34543a92d68238188debe": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_782cb24242ac46efbafebf9b9e06ad69",
              "IPY_MODEL_5189f3534bdc4f529e97c1e4fe0dc503",
              "IPY_MODEL_cc401990c419402c976ec98c398209ae"
            ],
            "layout": "IPY_MODEL_8b30f6d84ace4538a0004bb9e61379df"
          }
        },
        "782cb24242ac46efbafebf9b9e06ad69": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4a49f47b8bb14a8096f70815e31fceb0",
            "placeholder": "​",
            "style": "IPY_MODEL_d61610e45c544b9b94d9b72ffacb6175",
            "value": "pytorch_model.bin: 100%"
          }
        },
        "5189f3534bdc4f529e97c1e4fe0dc503": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6b6c3fc70a5842398278d2d7296b208f",
            "max": 1018571383,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_89893cc82ad7479ab3d5539c03fbef63",
            "value": 1018571383
          }
        },
        "cc401990c419402c976ec98c398209ae": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_fca5a2f4f0ae4e729c993d9f38fefa04",
            "placeholder": "​",
            "style": "IPY_MODEL_ef1e0a33d35947518e36f073031d7cb9",
            "value": " 1.02G/1.02G [00:05&lt;00:00, 195MB/s]"
          }
        },
        "8b30f6d84ace4538a0004bb9e61379df": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4a49f47b8bb14a8096f70815e31fceb0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d61610e45c544b9b94d9b72ffacb6175": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "6b6c3fc70a5842398278d2d7296b208f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "89893cc82ad7479ab3d5539c03fbef63": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "fca5a2f4f0ae4e729c993d9f38fefa04": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ef1e0a33d35947518e36f073031d7cb9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install opencv-python torchvision torchaudio ultralytics\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "IK8KpL-UIfVP",
        "outputId": "4f46967a-d2bf-4145-dccc-23201e832f6d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: opencv-python in /usr/local/lib/python3.10/dist-packages (4.10.0.84)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (0.19.0+cu121)\n",
            "Requirement already satisfied: torchaudio in /usr/local/lib/python3.10/dist-packages (2.4.0+cu121)\n",
            "Collecting ultralytics\n",
            "  Downloading ultralytics-8.2.91-py3-none-any.whl.metadata (41 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m41.9/41.9 kB\u001b[0m \u001b[31m1.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.21.2 in /usr/local/lib/python3.10/dist-packages (from opencv-python) (1.26.4)\n",
            "Requirement already satisfied: torch==2.4.0 in /usr/local/lib/python3.10/dist-packages (from torchvision) (2.4.0+cu121)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision) (9.4.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch==2.4.0->torchvision) (3.15.4)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch==2.4.0->torchvision) (4.12.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch==2.4.0->torchvision) (1.13.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch==2.4.0->torchvision) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch==2.4.0->torchvision) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch==2.4.0->torchvision) (2024.6.1)\n",
            "Requirement already satisfied: matplotlib>=3.3.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (3.7.1)\n",
            "Requirement already satisfied: pyyaml>=5.3.1 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (6.0.2)\n",
            "Requirement already satisfied: requests>=2.23.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (2.32.3)\n",
            "Requirement already satisfied: scipy>=1.4.1 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (1.13.1)\n",
            "Requirement already satisfied: tqdm>=4.64.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (4.66.5)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from ultralytics) (5.9.5)\n",
            "Requirement already satisfied: py-cpuinfo in /usr/local/lib/python3.10/dist-packages (from ultralytics) (9.0.0)\n",
            "Requirement already satisfied: pandas>=1.1.4 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (2.1.4)\n",
            "Requirement already satisfied: seaborn>=0.11.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (0.13.1)\n",
            "Collecting ultralytics-thop>=2.0.0 (from ultralytics)\n",
            "  Downloading ultralytics_thop-2.0.6-py3-none-any.whl.metadata (9.1 kB)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.0->ultralytics) (1.3.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.0->ultralytics) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.0->ultralytics) (4.53.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.0->ultralytics) (1.4.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.0->ultralytics) (24.1)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.0->ultralytics) (3.1.4)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.0->ultralytics) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.1.4->ultralytics) (2024.1)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.1.4->ultralytics) (2024.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.23.0->ultralytics) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.23.0->ultralytics) (3.8)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.23.0->ultralytics) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.23.0->ultralytics) (2024.8.30)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib>=3.3.0->ultralytics) (1.16.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch==2.4.0->torchvision) (2.1.5)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch==2.4.0->torchvision) (1.3.0)\n",
            "Downloading ultralytics-8.2.91-py3-none-any.whl (871 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m871.8/871.8 kB\u001b[0m \u001b[31m15.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading ultralytics_thop-2.0.6-py3-none-any.whl (26 kB)\n",
            "Installing collected packages: ultralytics-thop, ultralytics\n",
            "Successfully installed ultralytics-8.2.91 ultralytics-thop-2.0.6\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers\n"
      ],
      "metadata": {
        "collapsed": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fHhozsepQaoQ",
        "outputId": "d019dd9b-d7bb-40b0-aa31-b95f508c4d0b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.44.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.15.4)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.24.6)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.5.15)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.4)\n",
            "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.19.1)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.5)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (2024.6.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.8)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2024.8.30)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "YiC62MziFiW0",
        "outputId": "455901b4-5e0b-4ad5-b1ff-25c29fe8b2a0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "0: 640x640 (no detections), 16.4ms\n",
            "Speed: 1.7ms preprocess, 16.4ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
            "\n",
            "0: 640x640 1 scissors, 16.5ms\n",
            "Speed: 3.7ms preprocess, 16.5ms inference, 635.2ms postprocess per image at shape (1, 3, 640, 640)\n",
            "\n",
            "0: 640x640 1 scissors, 16.5ms\n",
            "Speed: 2.1ms preprocess, 16.5ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 640)\n",
            "\n",
            "0: 640x640 1 car, 16.6ms\n",
            "Speed: 1.9ms preprocess, 16.6ms inference, 1.3ms postprocess per image at shape (1, 3, 640, 640)\n",
            "\n",
            "0: 640x640 1 car, 1 truck, 16.6ms\n",
            "Speed: 2.0ms preprocess, 16.6ms inference, 1.6ms postprocess per image at shape (1, 3, 640, 640)\n",
            "\n",
            "0: 640x640 (no detections), 16.6ms\n",
            "Speed: 2.0ms preprocess, 16.6ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
            "\n",
            "0: 640x640 1 car, 16.8ms\n",
            "Speed: 2.7ms preprocess, 16.8ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 640)\n",
            "\n",
            "0: 640x640 1 car, 16.5ms\n",
            "Speed: 2.8ms preprocess, 16.5ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 640)\n",
            "\n",
            "0: 640x640 2 cars, 16.6ms\n",
            "Speed: 2.1ms preprocess, 16.6ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 640)\n",
            "\n",
            "0: 640x640 1 car, 16.7ms\n",
            "Speed: 2.1ms preprocess, 16.7ms inference, 1.3ms postprocess per image at shape (1, 3, 640, 640)\n",
            "\n",
            "0: 640x640 1 car, 16.5ms\n",
            "Speed: 2.2ms preprocess, 16.5ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 640)\n",
            "\n",
            "0: 640x640 1 car, 16.5ms\n",
            "Speed: 3.5ms preprocess, 16.5ms inference, 1.8ms postprocess per image at shape (1, 3, 640, 640)\n",
            "\n",
            "0: 640x640 (no detections), 16.5ms\n",
            "Speed: 3.7ms preprocess, 16.5ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
            "\n",
            "0: 640x640 1 cell phone, 16.5ms\n",
            "Speed: 1.6ms preprocess, 16.5ms inference, 1.6ms postprocess per image at shape (1, 3, 640, 640)\n",
            "\n",
            "0: 640x640 1 cell phone, 16.5ms\n",
            "Speed: 2.5ms preprocess, 16.5ms inference, 1.3ms postprocess per image at shape (1, 3, 640, 640)\n",
            "\n",
            "0: 640x640 2 cars, 1 bus, 16.6ms\n",
            "Speed: 2.1ms preprocess, 16.6ms inference, 1.3ms postprocess per image at shape (1, 3, 640, 640)\n",
            "\n",
            "0: 640x640 2 cars, 16.7ms\n",
            "Speed: 2.1ms preprocess, 16.7ms inference, 1.3ms postprocess per image at shape (1, 3, 640, 640)\n",
            "\n",
            "0: 640x640 1 vase, 16.5ms\n",
            "Speed: 2.1ms preprocess, 16.5ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 640)\n",
            "\n",
            "0: 640x640 1 person, 16.5ms\n",
            "Speed: 3.2ms preprocess, 16.5ms inference, 1.3ms postprocess per image at shape (1, 3, 640, 640)\n",
            "\n",
            "0: 640x640 (no detections), 16.6ms\n",
            "Speed: 2.8ms preprocess, 16.6ms inference, 0.9ms postprocess per image at shape (1, 3, 640, 640)\n",
            "\n",
            "0: 640x640 1 person, 17.6ms\n",
            "Speed: 2.2ms preprocess, 17.6ms inference, 1.8ms postprocess per image at shape (1, 3, 640, 640)\n",
            "\n",
            "0: 640x640 1 clock, 16.5ms\n",
            "Speed: 2.2ms preprocess, 16.5ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 640)\n",
            "\n",
            "0: 640x640 (no detections), 21.2ms\n",
            "Speed: 2.0ms preprocess, 21.2ms inference, 0.8ms postprocess per image at shape (1, 3, 640, 640)\n",
            "\n",
            "0: 640x640 2 persons, 1 bottle, 22.8ms\n",
            "Speed: 2.1ms preprocess, 22.8ms inference, 1.6ms postprocess per image at shape (1, 3, 640, 640)\n",
            "\n",
            "0: 640x640 (no detections), 16.5ms\n",
            "Speed: 3.5ms preprocess, 16.5ms inference, 0.8ms postprocess per image at shape (1, 3, 640, 640)\n",
            "\n",
            "0: 640x640 (no detections), 16.6ms\n",
            "Speed: 2.1ms preprocess, 16.6ms inference, 0.8ms postprocess per image at shape (1, 3, 640, 640)\n",
            "\n",
            "0: 640x640 (no detections), 16.5ms\n",
            "Speed: 3.5ms preprocess, 16.5ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
            "\n",
            "0: 640x640 (no detections), 16.6ms\n",
            "Speed: 2.2ms preprocess, 16.6ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
            "\n",
            "0: 640x640 1 parking meter, 16.6ms\n",
            "Speed: 2.2ms preprocess, 16.6ms inference, 1.8ms postprocess per image at shape (1, 3, 640, 640)\n",
            "\n",
            "0: 640x640 (no detections), 16.8ms\n",
            "Speed: 3.5ms preprocess, 16.8ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
            "\n",
            "0: 640x640 1 cell phone, 16.5ms\n",
            "Speed: 2.7ms preprocess, 16.5ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 640)\n",
            "\n",
            "0: 640x640 1 person, 1 car, 1 cell phone, 16.6ms\n",
            "Speed: 3.3ms preprocess, 16.6ms inference, 1.8ms postprocess per image at shape (1, 3, 640, 640)\n",
            "\n",
            "0: 640x640 1 airplane, 18.3ms\n",
            "Speed: 3.4ms preprocess, 18.3ms inference, 1.7ms postprocess per image at shape (1, 3, 640, 640)\n",
            "\n",
            "0: 640x640 1 car, 1 bus, 20.0ms\n",
            "Speed: 2.1ms preprocess, 20.0ms inference, 1.8ms postprocess per image at shape (1, 3, 640, 640)\n",
            "\n",
            "0: 640x640 1 parking meter, 18.1ms\n",
            "Speed: 4.0ms preprocess, 18.1ms inference, 1.9ms postprocess per image at shape (1, 3, 640, 640)\n",
            "\n",
            "0: 640x640 1 car, 16.6ms\n",
            "Speed: 2.9ms preprocess, 16.6ms inference, 2.1ms postprocess per image at shape (1, 3, 640, 640)\n",
            "\n",
            "0: 640x640 1 car, 18.3ms\n",
            "Speed: 3.2ms preprocess, 18.3ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 640)\n",
            "\n",
            "0: 640x640 1 car, 1 truck, 16.8ms\n",
            "Speed: 2.1ms preprocess, 16.8ms inference, 1.3ms postprocess per image at shape (1, 3, 640, 640)\n",
            "\n",
            "0: 640x640 1 car, 1 truck, 16.6ms\n",
            "Speed: 2.2ms preprocess, 16.6ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 640)\n",
            "\n",
            "0: 640x640 1 vase, 16.5ms\n",
            "Speed: 2.3ms preprocess, 16.5ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 640)\n",
            "\n",
            "0: 640x640 1 airplane, 16.5ms\n",
            "Speed: 2.4ms preprocess, 16.5ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 640)\n",
            "\n",
            "0: 640x640 2 birds, 16.5ms\n",
            "Speed: 2.5ms preprocess, 16.5ms inference, 1.3ms postprocess per image at shape (1, 3, 640, 640)\n",
            "\n",
            "0: 640x640 (no detections), 17.3ms\n",
            "Speed: 2.2ms preprocess, 17.3ms inference, 0.8ms postprocess per image at shape (1, 3, 640, 640)\n",
            "\n",
            "0: 640x640 1 clock, 17.0ms\n",
            "Speed: 2.5ms preprocess, 17.0ms inference, 1.3ms postprocess per image at shape (1, 3, 640, 640)\n",
            "\n",
            "0: 640x640 1 truck, 16.7ms\n",
            "Speed: 2.5ms preprocess, 16.7ms inference, 1.6ms postprocess per image at shape (1, 3, 640, 640)\n",
            "\n",
            "0: 640x640 1 car, 16.5ms\n",
            "Speed: 2.1ms preprocess, 16.5ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 640)\n",
            "\n",
            "0: 640x640 1 train, 16.6ms\n",
            "Speed: 2.0ms preprocess, 16.6ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 640)\n",
            "\n",
            "0: 640x640 1 car, 1 truck, 16.6ms\n",
            "Speed: 2.8ms preprocess, 16.6ms inference, 1.3ms postprocess per image at shape (1, 3, 640, 640)\n",
            "\n",
            "0: 640x640 (no detections), 16.5ms\n",
            "Speed: 2.0ms preprocess, 16.5ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
            "\n",
            "0: 640x640 1 car, 16.6ms\n",
            "Speed: 2.2ms preprocess, 16.6ms inference, 1.3ms postprocess per image at shape (1, 3, 640, 640)\n",
            "\n",
            "0: 640x640 1 car, 1 motorcycle, 16.6ms\n",
            "Speed: 2.1ms preprocess, 16.6ms inference, 1.6ms postprocess per image at shape (1, 3, 640, 640)\n",
            "\n",
            "0: 640x640 (no detections), 16.5ms\n",
            "Speed: 2.2ms preprocess, 16.5ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
            "\n",
            "0: 640x640 1 car, 16.5ms\n",
            "Speed: 3.4ms preprocess, 16.5ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 640)\n",
            "\n",
            "0: 640x640 1 car, 16.5ms\n",
            "Speed: 2.2ms preprocess, 16.5ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 640)\n",
            "\n",
            "0: 640x640 1 car, 1 truck, 16.6ms\n",
            "Speed: 3.0ms preprocess, 16.6ms inference, 1.9ms postprocess per image at shape (1, 3, 640, 640)\n",
            "\n",
            "0: 640x640 (no detections), 16.5ms\n",
            "Speed: 3.8ms preprocess, 16.5ms inference, 0.8ms postprocess per image at shape (1, 3, 640, 640)\n",
            "\n",
            "0: 640x640 (no detections), 16.5ms\n",
            "Speed: 3.1ms preprocess, 16.5ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
            "\n",
            "0: 640x640 (no detections), 16.5ms\n",
            "Speed: 3.2ms preprocess, 16.5ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
            "\n",
            "0: 640x640 1 clock, 16.4ms\n",
            "Speed: 2.0ms preprocess, 16.4ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 640)\n",
            "\n",
            "0: 640x640 (no detections), 16.5ms\n",
            "Speed: 2.0ms preprocess, 16.5ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
            "\n",
            "0: 640x640 (no detections), 16.5ms\n",
            "Speed: 2.2ms preprocess, 16.5ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n"
          ]
        }
      ],
      "source": [
        "import cv2\n",
        "import torch\n",
        "from ultralytics import YOLO\n",
        "import json\n",
        "\n",
        "# Load YOLOv8 model\n",
        "yolo_model = YOLO('yolov8s.pt')\n",
        "\n",
        "# Open video file\n",
        "video_path = '/content/The Performance-oriented ŠKODA SLAVIA 1.5 L TSI - A Class of its Own (1).mp4'\n",
        "cap = cv2.VideoCapture(video_path)\n",
        "\n",
        "# Get video properties\n",
        "fps = cap.get(cv2.CAP_PROP_FPS)\n",
        "frame_interval = int(fps)  # Extract one frame per second\n",
        "\n",
        "# YOLOv8 expects input images to be resized to 640x640 by default\n",
        "input_size = (640, 640)\n",
        "\n",
        "detections = []\n",
        "\n",
        "frame_count = 0\n",
        "while cap.isOpened():\n",
        "    ret, frame = cap.read()\n",
        "    if not ret:\n",
        "        break\n",
        "\n",
        "    if frame_count % frame_interval == 0:\n",
        "        # Resize the frame to the input size expected by YOLOv8\n",
        "        resized_frame = cv2.resize(frame, input_size)\n",
        "\n",
        "        # Perform object detection\n",
        "        results = yolo_model(resized_frame)\n",
        "\n",
        "        # Extract detection data with threshold of 0.6\n",
        "        frame_detections = []\n",
        "        for result in results[0].boxes:\n",
        "            bbox = result.xyxy[0].tolist()\n",
        "            confidence = result.conf[0].item()\n",
        "            class_id = int(result.cls[0].item())\n",
        "\n",
        "            if confidence >= 0.6:\n",
        "                frame_detections.append({\n",
        "                    'bbox': bbox,\n",
        "                    'confidence': confidence,\n",
        "                    'class': class_id\n",
        "                })\n",
        "\n",
        "        # Store results\n",
        "        detections.append({\n",
        "            'frame': int(cap.get(cv2.CAP_PROP_POS_FRAMES)),\n",
        "            'detections': frame_detections\n",
        "        })\n",
        "\n",
        "    frame_count += 1\n",
        "\n",
        "cap.release()\n",
        "\n",
        "# Save detections to a file\n",
        "with open('detections.json', 'w') as f:\n",
        "    json.dump(detections, f, indent=4)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "import torch\n",
        "from transformers import BartTokenizer, BartForConditionalGeneration\n",
        "import json\n",
        "from PIL import Image\n",
        "import io\n",
        "\n",
        "# Load YOLOv8 medium model\n",
        "from ultralytics import YOLO\n",
        "yolo_model = YOLO('yolov8m.pt')\n",
        "\n",
        "# Define class names (this list should match the class IDs used by your YOLO model)\n",
        "class_names = [\n",
        "    'person', 'bicycle', 'car', 'motorcycle', 'airplane', 'bus', 'train', 'truck', 'boat', 'traffic light',\n",
        "    'fire hydrant', 'stop sign', 'parking meter', 'bench', 'bird', 'cat', 'dog', 'horse', 'sheep', 'cow',\n",
        "    'elephant', 'bear', 'zebra', 'giraffe', 'backpack', 'umbrella', 'handbag', 'tie', 'suitcase', 'frisbee',\n",
        "    'skis', 'snowboard', 'sports ball', 'kite', 'baseball bat', 'baseball glove', 'skateboard', 'surfboard',\n",
        "    'tennis racket', 'bottle', 'wine glass', 'cup', 'fork', 'knife', 'spoon', 'bowl', 'banana', 'apple',\n",
        "    'sandwich', 'orange', 'broccoli', 'carrot', 'hot dog', 'pizza', 'donut', 'cake', 'chair', 'couch',\n",
        "    'potted plant', 'bed', 'dining table', 'toilet', 'tv', 'laptop', 'mouse', 'remote', 'keyboard', 'cell phone',\n",
        "    'microwave', 'oven', 'toaster', 'sink', 'refrigerator', 'book', 'clock', 'vase', 'scissors', 'teddy bear',\n",
        "    'hair drier', 'toothbrush'\n",
        "]\n",
        "\n",
        "# Load BART model and tokenizer\n",
        "tokenizer = BartTokenizer.from_pretrained('facebook/bart-large')\n",
        "bart_model = BartForConditionalGeneration.from_pretrained('facebook/bart-large')\n",
        "\n",
        "def generate_description(detections, image):\n",
        "    # Convert image to a format suitable for BART (e.g., base64 or text description)\n",
        "    buffered = io.BytesIO()\n",
        "    image.save(buffered, format=\"JPEG\")\n",
        "    image_str = buffered.getvalue()\n",
        "\n",
        "    # Create a description input for BART\n",
        "    description_input = f\"Frame image data: {image_str}\\nDetections: {json.dumps(detections)}\"\n",
        "\n",
        "    # Tokenize and generate description\n",
        "    inputs = tokenizer(description_input, return_tensors=\"pt\", max_length=1024, truncation=True)\n",
        "    summary_ids = bart_model.generate(inputs[\"input_ids\"], max_length=150, min_length=30, length_penalty=2.0, num_beams=4)\n",
        "    description = tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
        "\n",
        "    return description\n",
        "\n",
        "# Open video file\n",
        "video_path = '/content/The Performance-oriented ŠKODA SLAVIA 1.5 L TSI - A Class of its Own (1).mp4'\n",
        "cap = cv2.VideoCapture(video_path)\n",
        "\n",
        "# Get video properties\n",
        "fps = cap.get(cv2.CAP_PROP_FPS)\n",
        "frame_interval = int(fps)  # Extract 1 frame per second\n",
        "\n",
        "print(f\"Video FPS: {fps}\")\n",
        "print(f\"Frame Interval: {frame_interval}\")\n",
        "\n",
        "# Lists to store frames and detections\n",
        "frame_list = []\n",
        "detections_list = []\n",
        "\n",
        "# Initialize sequential frame counter\n",
        "sequential_frame_count = 1\n",
        "\n",
        "frame_count = 0\n",
        "while cap.isOpened():\n",
        "    ret, frame = cap.read()\n",
        "    if not ret:\n",
        "        print(\"End of video or error reading frame.\")\n",
        "        break\n",
        "\n",
        "    # Extract 1 frame per second\n",
        "    if frame_count % frame_interval == 0:\n",
        "        frame_list.append(frame)\n",
        "\n",
        "        # Resize the frame to the input size expected by YOLOv8\n",
        "        resized_frame = cv2.resize(frame, (640, 640))\n",
        "\n",
        "        # Perform object detection\n",
        "        results = yolo_model(resized_frame)\n",
        "\n",
        "        # Extract detection data with threshold of 0.7\n",
        "        frame_detections = []\n",
        "        for result in results[0].boxes:\n",
        "            bbox = result.xyxy[0].tolist()\n",
        "            confidence = result.conf[0].item()\n",
        "            class_id = int(result.cls[0].item())\n",
        "            class_name = class_names[class_id] if class_id < len(class_names) else 'unknown'\n",
        "\n",
        "            if confidence >= 0.7:\n",
        "                frame_detections.append({\n",
        "                    'bbox': bbox,\n",
        "                    'confidence': confidence,\n",
        "                    'class': class_name\n",
        "                })\n",
        "\n",
        "        # Convert frame to PIL Image\n",
        "        image = Image.fromarray(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB))\n",
        "\n",
        "        # Generate description\n",
        "        #description = generate_description(frame_detections, image)\n",
        "        print(f\"Frame {sequential_frame_count}\")\n",
        "\n",
        "        # Store detections with sequential frame numbering\n",
        "        detections_list.append({\n",
        "            'frame': sequential_frame_count,\n",
        "            'detections': frame_detections,\n",
        "        })\n",
        "\n",
        "        # Increment the sequential frame counter\n",
        "        sequential_frame_count += 1\n",
        "\n",
        "    frame_count += 1\n",
        "\n",
        "cap.release()\n",
        "\n",
        "# Save detections to JSON file\n",
        "with open('detections.json', 'w') as f:\n",
        "    json.dump(detections_list, f, indent=4)\n",
        "\n",
        "print(\"Detections saved to detections.json\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "x1bKzIiUOssC",
        "outputId": "7bb13b5b-a283-42a1-b7d3-53940483f705"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Video FPS: 25.0\n",
            "Frame Interval: 25\n",
            "\n",
            "0: 640x640 (no detections), 37.3ms\n",
            "Speed: 2.2ms preprocess, 37.3ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 640)\n",
            "Frame 1\n",
            "\n",
            "0: 640x640 1 scissors, 37.4ms\n",
            "Speed: 2.8ms preprocess, 37.4ms inference, 1.9ms postprocess per image at shape (1, 3, 640, 640)\n",
            "Frame 2\n",
            "\n",
            "0: 640x640 1 scissors, 37.4ms\n",
            "Speed: 2.3ms preprocess, 37.4ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 640)\n",
            "Frame 3\n",
            "\n",
            "0: 640x640 1 car, 37.3ms\n",
            "Speed: 3.5ms preprocess, 37.3ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 640)\n",
            "Frame 4\n",
            "\n",
            "0: 640x640 1 car, 37.3ms\n",
            "Speed: 3.5ms preprocess, 37.3ms inference, 1.3ms postprocess per image at shape (1, 3, 640, 640)\n",
            "Frame 5\n",
            "\n",
            "0: 640x640 1 truck, 37.3ms\n",
            "Speed: 3.1ms preprocess, 37.3ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 640)\n",
            "Frame 6\n",
            "\n",
            "0: 640x640 1 person, 1 car, 37.3ms\n",
            "Speed: 2.6ms preprocess, 37.3ms inference, 1.7ms postprocess per image at shape (1, 3, 640, 640)\n",
            "Frame 7\n",
            "\n",
            "0: 640x640 1 person, 1 car, 37.3ms\n",
            "Speed: 2.7ms preprocess, 37.3ms inference, 1.6ms postprocess per image at shape (1, 3, 640, 640)\n",
            "Frame 8\n",
            "\n",
            "0: 640x640 1 person, 2 cars, 37.3ms\n",
            "Speed: 1.7ms preprocess, 37.3ms inference, 1.3ms postprocess per image at shape (1, 3, 640, 640)\n",
            "Frame 9\n",
            "\n",
            "0: 640x640 1 car, 37.3ms\n",
            "Speed: 2.2ms preprocess, 37.3ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 640)\n",
            "Frame 10\n",
            "\n",
            "0: 640x640 1 car, 37.3ms\n",
            "Speed: 2.3ms preprocess, 37.3ms inference, 1.3ms postprocess per image at shape (1, 3, 640, 640)\n",
            "Frame 11\n",
            "\n",
            "0: 640x640 1 car, 37.5ms\n",
            "Speed: 2.4ms preprocess, 37.5ms inference, 2.5ms postprocess per image at shape (1, 3, 640, 640)\n",
            "Frame 12\n",
            "\n",
            "0: 640x640 (no detections), 37.3ms\n",
            "Speed: 2.7ms preprocess, 37.3ms inference, 0.9ms postprocess per image at shape (1, 3, 640, 640)\n",
            "Frame 13\n",
            "\n",
            "0: 640x640 1 train, 1 cell phone, 37.3ms\n",
            "Speed: 2.4ms preprocess, 37.3ms inference, 1.3ms postprocess per image at shape (1, 3, 640, 640)\n",
            "Frame 14\n",
            "\n",
            "0: 640x640 (no detections), 37.4ms\n",
            "Speed: 3.8ms preprocess, 37.4ms inference, 0.8ms postprocess per image at shape (1, 3, 640, 640)\n",
            "Frame 15\n",
            "\n",
            "0: 640x640 2 cars, 37.3ms\n",
            "Speed: 2.1ms preprocess, 37.3ms inference, 1.3ms postprocess per image at shape (1, 3, 640, 640)\n",
            "Frame 16\n",
            "\n",
            "0: 640x640 2 cars, 1 parking meter, 37.6ms\n",
            "Speed: 2.3ms preprocess, 37.6ms inference, 1.3ms postprocess per image at shape (1, 3, 640, 640)\n",
            "Frame 17\n",
            "\n",
            "0: 640x640 1 vase, 37.3ms\n",
            "Speed: 2.2ms preprocess, 37.3ms inference, 2.2ms postprocess per image at shape (1, 3, 640, 640)\n",
            "Frame 18\n",
            "\n",
            "0: 640x640 2 persons, 37.3ms\n",
            "Speed: 2.4ms preprocess, 37.3ms inference, 1.3ms postprocess per image at shape (1, 3, 640, 640)\n",
            "Frame 19\n",
            "\n",
            "0: 640x640 (no detections), 37.3ms\n",
            "Speed: 2.9ms preprocess, 37.3ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
            "Frame 20\n",
            "\n",
            "0: 640x640 1 person, 37.3ms\n",
            "Speed: 3.3ms preprocess, 37.3ms inference, 1.3ms postprocess per image at shape (1, 3, 640, 640)\n",
            "Frame 21\n",
            "\n",
            "0: 640x640 1 clock, 38.2ms\n",
            "Speed: 2.3ms preprocess, 38.2ms inference, 1.3ms postprocess per image at shape (1, 3, 640, 640)\n",
            "Frame 22\n",
            "\n",
            "0: 640x640 1 clock, 37.3ms\n",
            "Speed: 2.5ms preprocess, 37.3ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 640)\n",
            "Frame 23\n",
            "\n",
            "0: 640x640 2 persons, 37.4ms\n",
            "Speed: 2.8ms preprocess, 37.4ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 640)\n",
            "Frame 24\n",
            "\n",
            "0: 640x640 (no detections), 37.3ms\n",
            "Speed: 2.5ms preprocess, 37.3ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
            "Frame 25\n",
            "\n",
            "0: 640x640 1 clock, 37.3ms\n",
            "Speed: 2.4ms preprocess, 37.3ms inference, 1.3ms postprocess per image at shape (1, 3, 640, 640)\n",
            "Frame 26\n",
            "\n",
            "0: 640x640 1 clock, 37.3ms\n",
            "Speed: 2.6ms preprocess, 37.3ms inference, 1.7ms postprocess per image at shape (1, 3, 640, 640)\n",
            "Frame 27\n",
            "\n",
            "0: 640x640 1 cat, 37.3ms\n",
            "Speed: 3.6ms preprocess, 37.3ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 640)\n",
            "Frame 28\n",
            "\n",
            "0: 640x640 1 scissors, 37.3ms\n",
            "Speed: 3.5ms preprocess, 37.3ms inference, 1.3ms postprocess per image at shape (1, 3, 640, 640)\n",
            "Frame 29\n",
            "\n",
            "0: 640x640 (no detections), 37.3ms\n",
            "Speed: 2.4ms preprocess, 37.3ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
            "Frame 30\n",
            "\n",
            "0: 640x640 1 train, 1 cell phone, 37.4ms\n",
            "Speed: 2.5ms preprocess, 37.4ms inference, 1.6ms postprocess per image at shape (1, 3, 640, 640)\n",
            "Frame 31\n",
            "\n",
            "0: 640x640 1 person, 1 car, 1 cell phone, 37.4ms\n",
            "Speed: 2.5ms preprocess, 37.4ms inference, 1.3ms postprocess per image at shape (1, 3, 640, 640)\n",
            "Frame 32\n",
            "\n",
            "0: 640x640 1 bus, 37.3ms\n",
            "Speed: 3.8ms preprocess, 37.3ms inference, 1.3ms postprocess per image at shape (1, 3, 640, 640)\n",
            "Frame 33\n",
            "\n",
            "0: 640x640 1 car, 1 truck, 37.6ms\n",
            "Speed: 2.3ms preprocess, 37.6ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 640)\n",
            "Frame 34\n",
            "\n",
            "0: 640x640 1 truck, 1 parking meter, 37.5ms\n",
            "Speed: 2.3ms preprocess, 37.5ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 640)\n",
            "Frame 35\n",
            "\n",
            "0: 640x640 1 car, 37.3ms\n",
            "Speed: 2.6ms preprocess, 37.3ms inference, 1.3ms postprocess per image at shape (1, 3, 640, 640)\n",
            "Frame 36\n",
            "\n",
            "0: 640x640 1 car, 37.3ms\n",
            "Speed: 2.2ms preprocess, 37.3ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 640)\n",
            "Frame 37\n",
            "\n",
            "0: 640x640 1 car, 37.3ms\n",
            "Speed: 2.3ms preprocess, 37.3ms inference, 1.3ms postprocess per image at shape (1, 3, 640, 640)\n",
            "Frame 38\n",
            "\n",
            "0: 640x640 1 car, 1 truck, 37.3ms\n",
            "Speed: 2.3ms preprocess, 37.3ms inference, 1.7ms postprocess per image at shape (1, 3, 640, 640)\n",
            "Frame 39\n",
            "\n",
            "0: 640x640 (no detections), 37.4ms\n",
            "Speed: 2.6ms preprocess, 37.4ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
            "Frame 40\n",
            "\n",
            "0: 640x640 1 airplane, 37.3ms\n",
            "Speed: 3.1ms preprocess, 37.3ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 640)\n",
            "Frame 41\n",
            "\n",
            "0: 640x640 (no detections), 37.5ms\n",
            "Speed: 2.1ms preprocess, 37.5ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
            "Frame 42\n",
            "\n",
            "0: 640x640 1 clock, 37.3ms\n",
            "Speed: 2.2ms preprocess, 37.3ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 640)\n",
            "Frame 43\n",
            "\n",
            "0: 640x640 1 clock, 37.3ms\n",
            "Speed: 2.9ms preprocess, 37.3ms inference, 1.3ms postprocess per image at shape (1, 3, 640, 640)\n",
            "Frame 44\n",
            "\n",
            "0: 640x640 1 truck, 37.3ms\n",
            "Speed: 1.9ms preprocess, 37.3ms inference, 1.3ms postprocess per image at shape (1, 3, 640, 640)\n",
            "Frame 45\n",
            "\n",
            "0: 640x640 1 car, 37.6ms\n",
            "Speed: 2.2ms preprocess, 37.6ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 640)\n",
            "Frame 46\n",
            "\n",
            "0: 640x640 (no detections), 37.3ms\n",
            "Speed: 3.6ms preprocess, 37.3ms inference, 0.8ms postprocess per image at shape (1, 3, 640, 640)\n",
            "Frame 47\n",
            "\n",
            "0: 640x640 1 car, 37.3ms\n",
            "Speed: 2.3ms preprocess, 37.3ms inference, 1.6ms postprocess per image at shape (1, 3, 640, 640)\n",
            "Frame 48\n",
            "\n",
            "0: 640x640 (no detections), 37.3ms\n",
            "Speed: 3.2ms preprocess, 37.3ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
            "Frame 49\n",
            "\n",
            "0: 640x640 1 car, 37.3ms\n",
            "Speed: 2.6ms preprocess, 37.3ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 640)\n",
            "Frame 50\n",
            "\n",
            "0: 640x640 1 car, 37.3ms\n",
            "Speed: 2.7ms preprocess, 37.3ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 640)\n",
            "Frame 51\n",
            "\n",
            "0: 640x640 (no detections), 37.3ms\n",
            "Speed: 2.3ms preprocess, 37.3ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
            "Frame 52\n",
            "\n",
            "0: 640x640 1 car, 37.3ms\n",
            "Speed: 3.4ms preprocess, 37.3ms inference, 1.3ms postprocess per image at shape (1, 3, 640, 640)\n",
            "Frame 53\n",
            "\n",
            "0: 640x640 1 car, 37.3ms\n",
            "Speed: 3.4ms preprocess, 37.3ms inference, 1.3ms postprocess per image at shape (1, 3, 640, 640)\n",
            "Frame 54\n",
            "\n",
            "0: 640x640 1 car, 37.4ms\n",
            "Speed: 2.7ms preprocess, 37.4ms inference, 1.8ms postprocess per image at shape (1, 3, 640, 640)\n",
            "Frame 55\n",
            "\n",
            "0: 640x640 (no detections), 37.3ms\n",
            "Speed: 2.5ms preprocess, 37.3ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
            "Frame 56\n",
            "\n",
            "0: 640x640 (no detections), 37.4ms\n",
            "Speed: 2.4ms preprocess, 37.4ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
            "Frame 57\n",
            "\n",
            "0: 640x640 (no detections), 37.5ms\n",
            "Speed: 2.2ms preprocess, 37.5ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
            "Frame 58\n",
            "\n",
            "0: 640x640 1 clock, 37.3ms\n",
            "Speed: 3.0ms preprocess, 37.3ms inference, 1.3ms postprocess per image at shape (1, 3, 640, 640)\n",
            "Frame 59\n",
            "\n",
            "0: 640x640 (no detections), 37.3ms\n",
            "Speed: 3.5ms preprocess, 37.3ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
            "Frame 60\n",
            "\n",
            "0: 640x640 (no detections), 37.4ms\n",
            "Speed: 2.3ms preprocess, 37.4ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
            "Frame 61\n",
            "End of video or error reading frame.\n",
            "Detections saved to detections.json\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "from transformers import BartTokenizer, BartForConditionalGeneration\n",
        "import torch\n",
        "\n",
        "# Load BART model and tokenizer\n",
        "tokenizer = BartTokenizer.from_pretrained('facebook/bart-large')\n",
        "bart_model = BartForConditionalGeneration.from_pretrained('facebook/bart-large').to('cuda')\n",
        "\n",
        "def generate_description(detections_list):\n",
        "    # Format all detections from the video\n",
        "    formatted_detections = []\n",
        "    for entry in detections_list:\n",
        "        frame = entry.get('frame')\n",
        "        detections = entry.get('detections', [])\n",
        "        for detection in detections:\n",
        "            if 'bbox' in detection and 'confidence' in detection and 'class' in detection:\n",
        "                formatted_detections.append({\n",
        "                    'frame': frame,\n",
        "                    'bbox': detection['bbox'],\n",
        "                    'confidence': detection['confidence'],\n",
        "                    'class': detection['class']\n",
        "                })\n",
        "\n",
        "    # Create a single input for the entire video\n",
        "    description_input = f\"Video detections: {json.dumps(formatted_detections)}\"\n",
        "\n",
        "    # Tokenize and generate a single description for the whole video\n",
        "    inputs = tokenizer(description_input, return_tensors=\"pt\", max_length=1024, truncation=True).to('cuda')\n",
        "    summary_ids = bart_model.generate(inputs[\"input_ids\"], max_length=150, min_length=50, length_penalty=2.0, num_beams=4)\n",
        "    description = tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
        "\n",
        "    return description\n",
        "\n",
        "def process_detections_file(file_path):\n",
        "    # Load detections from JSON file\n",
        "    with open(file_path, 'r') as f:\n",
        "        detections_list = json.load(f)\n",
        "\n",
        "    # Generate a single description for the whole video\n",
        "    description = generate_description(detections_list)\n",
        "\n",
        "    return description\n",
        "\n",
        "# Example usage\n",
        "file_path = 'detections.json'\n",
        "video_description = process_detections_file(file_path)\n",
        "print(f\"Video Description: {video_description}\")\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 385,
          "referenced_widgets": [
            "da5cb3b1c6c140da96f367f4493734e9",
            "98cac349d45444faa5db226cbe82351c",
            "4022ad525b0c4faab0a8305e0fe2e6ee",
            "43f3ce7eed8e4e9a9fd5b54eda9a2c62",
            "058cd07698cd4e04a500c2d60d3ec2fa",
            "6d4944828fb84f4db262740f869dd904",
            "a82d8e090a6f47b780eeb82b2de4b9e9",
            "5bef0647f64f4a6aa8b1129492f86666",
            "08a7c4a3aff74157970b869624fc5108",
            "0ee20a580c594b2bac6686ce5700a87c",
            "d0e41ee0cb3b437ebdf95419deaa17c8",
            "6df92aceb9484ead8bddeb5b1c683107",
            "02166dc287cc42f29a15119bb0369100",
            "a3c108526ba74fd2b82ee9df8b87a12e",
            "a25a5c419d76480b90472fb9bd7de2d8",
            "a5b8d24f4fac46f0bf04a2972cd117f5",
            "8485fee50df74a8b9f182f337419fc92",
            "6f59d88949844f5991114af6fea5f149",
            "6a1be352061b40c69924fbcb0fc673d1",
            "de183776f76d4780a0656218b48b28df",
            "b3136ce438f74aeeab6bc11bf52db002",
            "e373234271a94233b77ca407da0fa921",
            "7458d5532f494d468e5853c314bd96ad",
            "7ed6986f9e7c4f75ab27079e492e44fb",
            "cbd3667fb68049f78c5d745896aaef65",
            "5c272dbaa2a84337b38a4b4d2fdfddfb",
            "5354473cf3d645668a81f783728b228d",
            "92d4b121a4f848f385bb904dfb8c571c",
            "ca5e30ae943a4c678d2d43b1f2245c2e",
            "2c79940bdece47a3bd5a3a3dbb07b329",
            "c2735f4281a642baac1b7b605dc428eb",
            "2930c178762340d3b9325970fc273c49",
            "e46c2f25f2e94e2787f560c2d6307840",
            "f4d33f579b59408381e3e46ace181dde",
            "294c8b4c39be4beab5c9170a76467f56",
            "faff09cb171a4bcc9ff3ad5c076a66d7",
            "bcde1c7b60fa4b7fb51ea37bbdbb90a5",
            "d896d50918ee4c8fa03c41bddb7ebfad",
            "1f548f90477046a990398de665a47f3c",
            "d80e8f5d076b47b582415ab0952c016a",
            "62ca4b80f0714c1983d1a35e31a88403",
            "0863dd793c9f4b3ebae3cbb02990261d",
            "10ef2edd55ba4ad9beabe87831103d92",
            "dbccb8cd7bb7440d9aafa22fc52c6d21",
            "762eed4f46e845549166d1b51c22608c",
            "6d7d9548fe314daa9cb9ab089b8f5e52",
            "c8c0219d5f06477f87a23e2d91c21fb8",
            "340b87f179264d6d927633af5b0e2e67",
            "772843cb165246b3abe661c61cf10fcf",
            "85980c6e960241f7b8beee5a39772acd",
            "9fa75e21194847c1a1337f937c892230",
            "2f8cdfce89994638b7befe7e6003c3e2",
            "55acdc60a2e746d3ada3e909d6afb780",
            "6873bb548b1e4c0f88ccd903daaea7cd",
            "1576511c15a24bfc869e4be59e5e0b48",
            "c4a94a7092a34543a92d68238188debe",
            "782cb24242ac46efbafebf9b9e06ad69",
            "5189f3534bdc4f529e97c1e4fe0dc503",
            "cc401990c419402c976ec98c398209ae",
            "8b30f6d84ace4538a0004bb9e61379df",
            "4a49f47b8bb14a8096f70815e31fceb0",
            "d61610e45c544b9b94d9b72ffacb6175",
            "6b6c3fc70a5842398278d2d7296b208f",
            "89893cc82ad7479ab3d5539c03fbef63",
            "fca5a2f4f0ae4e729c993d9f38fefa04",
            "ef1e0a33d35947518e36f073031d7cb9"
          ]
        },
        "id": "RBddTTdfeh4K",
        "outputId": "9054d53b-bd3c-4f15-ad13-dd13dd05ff31"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:89: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/26.0 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "da5cb3b1c6c140da96f367f4493734e9"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "vocab.json:   0%|          | 0.00/899k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "6df92aceb9484ead8bddeb5b1c683107"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "7458d5532f494d468e5853c314bd96ad"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "f4d33f579b59408381e3e46ace181dde"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.json:   0%|          | 0.00/1.63k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "762eed4f46e845549166d1b51c22608c"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "pytorch_model.bin:   0%|          | 0.00/1.02G [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "c4a94a7092a34543a92d68238188debe"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Video Description: Video detections: [{\"frame\": 4, \"bbox\": [36.71832275390625, 256.89569091796875, 455.08587646484375, 429.8542113304138184, \"class\": \"car\"}, {\"frame\": 5, \"featured\": [0.class62,125.bbox], 2352822265625, 282.83953857421875, 334.7838249206543, 146.6851801664775], \"confidence\": 0. {\"frame: 6, \"box\": 546.712418258190155, 192.76895141601562, 444\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "from transformers import BartTokenizer, BartForConditionalGeneration\n",
        "import torch\n",
        "\n",
        "# Load BART model and tokenizer\n",
        "tokenizer = BartTokenizer.from_pretrained('facebook/bart-large')\n",
        "bart_model = BartForConditionalGeneration.from_pretrained('facebook/bart-large').to('cuda')\n",
        "\n",
        "def clean_input(input_text):\n",
        "    # Remove unwanted characters or symbols\n",
        "    allowed_chars = ''.join(chr(i) for i in range(32, 127))  # ASCII characters from space to tilde (~)\n",
        "    cleaned_text = ''.join(c for c in input_text if c in allowed_chars)\n",
        "    return cleaned_text\n",
        "\n",
        "def generate_description(detections_summary):\n",
        "    # Ensure the input is cleaned before feeding it to the BART model\n",
        "    description_input = f\"The following objects were detected in the video:\\n{clean_input(detections_summary)}\"\n",
        "\n",
        "    # Tokenize and generate description\n",
        "    inputs = tokenizer(description_input, return_tensors=\"pt\", max_length=512, truncation=True).to('cuda')\n",
        "    summary_ids = bart_model.generate(inputs[\"input_ids\"], max_length=150, min_length=50, length_penalty=2.0, num_beams=4)\n",
        "    description = tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
        "\n",
        "    return description\n",
        "\n",
        "def process_detections_file(file_path):\n",
        "    # Load detections from JSON file\n",
        "    with open(file_path, 'r') as f:\n",
        "        detections_list = json.load(f)\n",
        "\n",
        "    # Collect detected objects along with their frame numbers\n",
        "    frame_objects = {}\n",
        "    for entry in detections_list:\n",
        "        frame = entry.get('frame')\n",
        "        detections = entry.get('detections', [])\n",
        "        objects_in_frame = []\n",
        "\n",
        "        for detection in detections:\n",
        "            if 'class' in detection:\n",
        "                class_name = detection['class']\n",
        "                objects_in_frame.append(class_name)\n",
        "\n",
        "        # Save objects detected in this frame\n",
        "        if objects_in_frame:\n",
        "            frame_objects[frame] = objects_in_frame\n",
        "\n",
        "    # Create a formatted summary\n",
        "    detections_summary = \"\"\n",
        "    for frame, objects in frame_objects.items():\n",
        "        objects_str = ', '.join(objects)\n",
        "        detections_summary += f\"Frame {frame}: {objects_str}\\n\"\n",
        "\n",
        "    # Generate a single description for the whole video\n",
        "    description = generate_description(detections_summary)\n",
        "\n",
        "    return description\n",
        "\n",
        "# Example usage\n",
        "file_path = 'detections.json'\n",
        "video_description = process_detections_file(file_path)\n",
        "print(f\"Video Description: {video_description}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tEjcJtaQH6zv",
        "outputId": "8078b4d4-adff-4088-9c2c-f4f895f8eb71"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Video Description: The following objects were detected in the video:Frame 1: car, personFrame 2: personFrame 3: person, carFrame 4: car frame 5: carFrame 6: person frame 7: person Frame 8: car Frame 9: car and personFrame 9: vehicleFrame 10: carFrames 11: personFrames 12: carframes 13: car framesFrame 14: vaseFrame 15: person.Frame 16: car.Frame 17: car\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "from transformers import BartTokenizer, BartForConditionalGeneration\n",
        "import torch\n",
        "\n",
        "# Load BART model and tokenizer\n",
        "tokenizer = BartTokenizer.from_pretrained('facebook/bart-large')\n",
        "bart_model = BartForConditionalGeneration.from_pretrained('facebook/bart-large').to('cuda')\n",
        "\n",
        "def clean_input(input_text):\n",
        "    # Remove unwanted characters or symbols\n",
        "    allowed_chars = ''.join(chr(i) for i in range(32, 127))  # ASCII characters from space to tilde (~)\n",
        "    cleaned_text = ''.join(c for c in input_text if c in allowed_chars)\n",
        "    return cleaned_text\n",
        "\n",
        "def generate_description(detections_summary):\n",
        "    # Ensure the input is cleaned before feeding it to the BART model\n",
        "    description_input = f\"The following objects and their positions were detected in the video:\\n{clean_input(detections_summary)}\"\n",
        "\n",
        "    # Tokenize and generate description\n",
        "    inputs = tokenizer(description_input, return_tensors=\"pt\", max_length=512, truncation=True).to('cuda')\n",
        "    summary_ids = bart_model.generate(inputs[\"input_ids\"], max_length=150, min_length=50, length_penalty=2.0, num_beams=4)\n",
        "    description = tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
        "\n",
        "    return description\n",
        "\n",
        "def process_detections_file(file_path):\n",
        "    # Load detections from JSON file\n",
        "    with open(file_path, 'r') as f:\n",
        "        detections_list = json.load(f)\n",
        "\n",
        "    # Sort the detection list by frame number to ensure sequential order\n",
        "    detections_list = sorted(detections_list, key=lambda x: x['frame'])\n",
        "\n",
        "    # Collect detected objects along with their frame numbers and positions (bounding boxes)\n",
        "    frame_objects = {}\n",
        "    for entry in detections_list:\n",
        "        frame = entry.get('frame')\n",
        "        detections = entry.get('detections', [])\n",
        "        objects_in_frame = []\n",
        "\n",
        "        for detection in detections:\n",
        "            if 'class' in detection and 'bbox' in detection:\n",
        "                class_name = detection['class']\n",
        "                bbox = detection['bbox']\n",
        "                # Convert bounding box to a readable format (e.g., top-left and bottom-right)\n",
        "                if len(bbox) == 4:  # Only process if the bounding box has the correct format\n",
        "                    bbox_str = f\"({bbox[0]:.1f}, {bbox[1]:.1f}, {bbox[2]:.1f}, {bbox[3]:.1f})\"\n",
        "                    objects_in_frame.append(f\"{class_name} at {bbox_str}\")\n",
        "\n",
        "        # Save objects detected in this frame\n",
        "        if objects_in_frame:\n",
        "            frame_objects[frame] = objects_in_frame\n",
        "\n",
        "    # Create a formatted summary including objects and their bounding box positions\n",
        "    detections_summary = \"\"\n",
        "    for frame, objects in frame_objects.items():\n",
        "        objects_str = '; '.join(objects)\n",
        "        detections_summary += f\"Frame {frame}: {objects_str}\\n\"\n",
        "\n",
        "    # Generate a single description for the whole video\n",
        "    description = generate_description(detections_summary)\n",
        "\n",
        "    return description\n",
        "\n",
        "# Example usage\n",
        "file_path = 'detections.json'\n",
        "video_description = process_detections_file(file_path)\n",
        "print(f\"Video Description: {video_description}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "afjNVxQkJQ64",
        "outputId": "1cbe675e-6696-4958-c20b-01afa3d572db"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Video Description: The following objects and their positions were detected in the video:Frame 4: car at (36.7, 84.9, 85.5, 83.6, 113.1, 135.7)Frame 6: person at (94.2, 139.8, 146.3, 150.7; car at 144.4, 157.4)Frame 18: vase at (204.2)Frame 23: car (169.1)Frame 32: person (168.1); car at 18.1; car (18.0); person at 1.7); car (0.0)Frame 8: car and truck at (304.0, 336.8; car(s)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "FJKS0M97VfsR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -U openai-whisper"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "5jonqegARDLk",
        "outputId": "e1c53c7d-67b7-4ed9-b518-2cbe023f2f05"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting openai-whisper\n",
            "  Downloading openai-whisper-20231117.tar.gz (798 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/798.6 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m266.2/798.6 kB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m \u001b[32m788.5/798.6 kB\u001b[0m \u001b[31m14.7 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m798.6/798.6 kB\u001b[0m \u001b[31m10.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting triton<3,>=2.0.0 (from openai-whisper)\n",
            "  Downloading triton-2.3.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.4 kB)\n",
            "Requirement already satisfied: numba in /usr/local/lib/python3.10/dist-packages (from openai-whisper) (0.60.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from openai-whisper) (1.26.4)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from openai-whisper) (2.4.0+cu121)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from openai-whisper) (4.66.5)\n",
            "Requirement already satisfied: more-itertools in /usr/local/lib/python3.10/dist-packages (from openai-whisper) (10.3.0)\n",
            "Collecting tiktoken (from openai-whisper)\n",
            "  Downloading tiktoken-0.7.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.6 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from triton<3,>=2.0.0->openai-whisper) (3.15.4)\n",
            "Requirement already satisfied: llvmlite<0.44,>=0.43.0dev0 in /usr/local/lib/python3.10/dist-packages (from numba->openai-whisper) (0.43.0)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.10/dist-packages (from tiktoken->openai-whisper) (2024.5.15)\n",
            "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.10/dist-packages (from tiktoken->openai-whisper) (2.32.3)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch->openai-whisper) (4.12.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch->openai-whisper) (1.13.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->openai-whisper) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->openai-whisper) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch->openai-whisper) (2024.6.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken->openai-whisper) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken->openai-whisper) (3.8)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken->openai-whisper) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken->openai-whisper) (2024.8.30)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->openai-whisper) (2.1.5)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch->openai-whisper) (1.3.0)\n",
            "Downloading triton-2.3.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (168.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m168.1/168.1 MB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tiktoken-0.7.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m43.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: openai-whisper\n",
            "  Building wheel for openai-whisper (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for openai-whisper: filename=openai_whisper-20231117-py3-none-any.whl size=801360 sha256=d1f70006406e18af96b18ed91a6c90b55112e42887470326f7bcd42552de614c\n",
            "  Stored in directory: /root/.cache/pip/wheels/d0/85/e1/9361b4cbea7dd4b7f6702fa4c3afc94877952eeb2b62f45f56\n",
            "Successfully built openai-whisper\n",
            "Installing collected packages: triton, tiktoken, openai-whisper\n",
            "Successfully installed openai-whisper-20231117 tiktoken-0.7.0 triton-2.3.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import whisper\n",
        "\n",
        "def transcribe_audio(audio_path, output_file):\n",
        "    model = whisper.load_model(\"medium\")\n",
        "    result = model.transcribe(audio_path, language=\"en\", temperature=0.6, verbose=True)\n",
        "\n",
        "    with open(output_file, 'w') as f:\n",
        "        for segment in result['segments']:\n",
        "            start_time = segment['start']\n",
        "            end_time = segment['end']\n",
        "            text = segment['text']\n",
        "            f.write(f\"[{start_time:.2f} - {end_time:.2f}] {text}\\n\")\n",
        "\n",
        "# Example usage\n",
        "audio_path = \"/content/audio file.m4a\"\n",
        "output_file = \"/content/transcription_with_timestamps.txt\"\n",
        "transcribe_audio(audio_path, output_file)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IiVNXsHLRB6c",
        "outputId": "e83533e1-51d3-47ee-d326-da2dd1793a10"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|█████████████████████████████████████| 1.42G/1.42G [00:20<00:00, 76.3MiB/s]\n",
            "/usr/local/lib/python3.10/dist-packages/whisper/__init__.py:146: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  checkpoint = torch.load(fp, map_location=device)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[00:00.000 --> 00:04.200]  There are many routes one can take in life.\n",
            "[00:04.200 --> 00:07.640]  Sometimes we take a beautiful\n",
            "[00:07.640 --> 00:10.920]  but measured journey.\n",
            "[00:10.920 --> 00:14.920]  But when life is full of choices...\n",
            "[00:14.920 --> 00:21.920]  sometimes you just let the heart take over.\n",
            "[00:30.000 --> 00:45.920]  When exhilaration drives you to aim higher.\n",
            "[00:45.920 --> 00:51.360]  When emotion overtakes all else.\n",
            "[00:51.360 --> 00:52.880]  ŠKODA SLAVIA 1.5 TSI\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "from transformers import BartTokenizer, BartForConditionalGeneration\n",
        "import torch\n",
        "\n",
        "# Load BART model and tokenizer\n",
        "tokenizer = BartTokenizer.from_pretrained('facebook/bart-large')\n",
        "bart_model = BartForConditionalGeneration.from_pretrained('facebook/bart-large')\n",
        "\n",
        "# Detect if GPU is available, otherwise use CPU\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "bart_model = bart_model.to(device)\n",
        "\n",
        "def clean_input(input_text):\n",
        "    # Remove unwanted characters or symbols\n",
        "    allowed_chars = ''.join(chr(i) for i in range(32, 127))  # ASCII characters from space to tilde (~)\n",
        "    cleaned_text = ''.join(c for c in input_text if c in allowed_chars)\n",
        "    return cleaned_text\n",
        "\n",
        "def generate_description(detections_summary, transcribed_text):\n",
        "    # Combine detections summary and transcribed text\n",
        "    description_input = (f\"Please generate a detailed textual summary of the video based on Detected Objects and Transcribed Audio\\n\\n\"\n",
        "                         f\"{clean_input(detections_summary)}\\n\\n\"\n",
        "                         f\"{clean_input(transcribed_text)}\\n\\n\")\n",
        "\n",
        "    # Debug: Print the final input to check formatting\n",
        "    print(\"Description Input:\\n\", description_input)\n",
        "\n",
        "    # Tokenize and generate description\n",
        "    inputs = tokenizer(description_input, return_tensors=\"pt\", max_length=1024, truncation=True).to(device)\n",
        "    summary_ids = bart_model.generate(inputs[\"input_ids\"], max_length=700, min_length=100, length_penalty=2.0, num_beams=4)\n",
        "    description = tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
        "\n",
        "    return description\n",
        "\n",
        "def process_detections_file(detections_file_path, transcribed_audio_file_path):\n",
        "    # Load detections from JSON file\n",
        "    with open(detections_file_path, 'r') as f:\n",
        "        detections_list = json.load(f)\n",
        "\n",
        "    # Load transcribed audio text from TXT file\n",
        "    with open(transcribed_audio_file_path, 'r') as f:\n",
        "        transcribed_text = f.read()\n",
        "\n",
        "    # Sort the detection list by frame number to ensure sequential order\n",
        "    detections_list = sorted(detections_list, key=lambda x: x['frame'])\n",
        "\n",
        "    # Collect detected objects along with their frame numbers and positions (bounding boxes)\n",
        "    frame_objects = {}\n",
        "    for entry in detections_list:\n",
        "        frame = entry.get('frame')\n",
        "        detections = entry.get('detections', [])\n",
        "        objects_in_frame = []\n",
        "\n",
        "        for detection in detections:\n",
        "            if 'class' in detection and 'bbox' in detection:\n",
        "                class_name = detection['class']\n",
        "                bbox = detection['bbox']\n",
        "                # Convert bounding box to a readable format (e.g., top-left and bottom-right)\n",
        "                if len(bbox) == 4:  # Only process if the bounding box has the correct format\n",
        "                    bbox_str = f\"({bbox[0]:.1f}, {bbox[1]:.1f}, {bbox[2]:.1f}, {bbox[3]:.1f})\"\n",
        "                    objects_in_frame.append(f\"{class_name} at {bbox_str}\")\n",
        "\n",
        "        # Save objects detected in this frame\n",
        "        if objects_in_frame:\n",
        "            frame_objects[frame] = objects_in_frame\n",
        "\n",
        "    # Create a formatted summary including objects and their bounding box positions\n",
        "    detections_summary = \"\"\n",
        "    for frame, objects in frame_objects.items():\n",
        "        objects_str = '; '.join(objects)\n",
        "        detections_summary += f\"Frame {frame}: {objects_str}\\n\"\n",
        "\n",
        "    # Debug: Print formatted summaries to check if they are correct\n",
        "    print(\"Detections Summary:\\n\", detections_summary)\n",
        "    print(\"Transcribed Text:\\n\", transcribed_text)\n",
        "\n",
        "    # Generate a single description for the whole video including transcribed audio\n",
        "    description = generate_description(detections_summary, transcribed_text)\n",
        "\n",
        "    return description\n",
        "\n",
        "# Example usage\n",
        "detections_file_path = 'detections.json'\n",
        "transcribed_audio_file_path = 'transcription_with_timestamps.txt'\n",
        "video_description = process_detections_file(detections_file_path, transcribed_audio_file_path)\n",
        "print(f\"Video Description: {video_description}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nu8KQpUtVg8n",
        "outputId": "e653c435-dc28-4433-86b1-e183a1c01ecb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Detections Summary:\n",
            " Frame 4: car at (36.8, 146.4, 157.4, 267.7)\n",
            "Frame 5: car at (94.7, 150.7, 220.0, 282.7)\n",
            "Frame 7: car at (287.3, 192.9, 455.1, 430.0)\n",
            "Frame 8: car at (304.8, 221.8, 444.2, 418.0); person at (377.3, 219.3, 382.7, 234.5)\n",
            "Frame 9: car at (289.4, 238.6, 423.5, 406.1); car at (169.8, 342.6, 224.5, 405.6)\n",
            "Frame 10: car at (174.7, 327.7, 256.7, 411.2)\n",
            "Frame 11: car at (192.3, 296.1, 337.0, 422.1)\n",
            "Frame 16: car at (55.4, 239.0, 460.2, 479.5); car at (443.8, 334.5, 562.1, 439.4)\n",
            "Frame 18: vase at (204.5, 83.4, 402.9, 397.4)\n",
            "Frame 19: person at (156.9, 83.8, 441.7, 551.2)\n",
            "Frame 24: person at (166.9, 85.7, 637.7, 546.1)\n",
            "Frame 32: person at (267.2, 139.9, 569.8, 553.1)\n",
            "Frame 36: car at (159.4, 235.8, 603.2, 500.9)\n",
            "Frame 38: car at (206.2, 193.8, 526.9, 505.6)\n",
            "Frame 45: truck at (189.5, 255.7, 251.9, 321.5)\n",
            "Frame 48: car at (259.1, 135.0, 336.2, 196.0)\n",
            "Frame 50: car at (18.6, 113.7, 441.9, 370.4)\n",
            "Frame 51: car at (0.7, 84.5, 160.8, 530.6)\n",
            "Frame 53: car at (216.6, 279.3, 393.8, 383.3)\n",
            "Frame 54: car at (298.8, 127.6, 466.3, 335.5)\n",
            "Frame 55: car at (0.5, 83.8, 273.8, 366.9)\n",
            "\n",
            "Transcribed Text:\n",
            " [0.00 - 4.20]  There are many routes one can take in life.\n",
            "[4.20 - 7.64]  Sometimes we take a beautiful\n",
            "[7.64 - 10.92]  but measured journey.\n",
            "[10.92 - 14.92]  But when life is full of choices...\n",
            "[14.92 - 21.92]  sometimes you just let the heart take over.\n",
            "[30.00 - 45.92]  When exhilaration drives you to aim higher.\n",
            "[45.92 - 51.36]  When emotion overtakes all else.\n",
            "[51.36 - 52.88]  ŠKODA SLAVIA 1.5 TSI\n",
            "\n",
            "Description Input:\n",
            " Please generate a detailed textual summary of the video based on Detected Objects and Transcribed Audio\n",
            "\n",
            "Frame 4: car at (36.8, 146.4, 157.4, 267.7)Frame 5: car at (94.7, 150.7, 220.0, 282.7)Frame 7: car at (287.3, 192.9, 455.1, 430.0)Frame 8: car at (304.8, 221.8, 444.2, 418.0); person at (377.3, 219.3, 382.7, 234.5)Frame 9: car at (289.4, 238.6, 423.5, 406.1); car at (169.8, 342.6, 224.5, 405.6)Frame 10: car at (174.7, 327.7, 256.7, 411.2)Frame 11: car at (192.3, 296.1, 337.0, 422.1)Frame 16: car at (55.4, 239.0, 460.2, 479.5); car at (443.8, 334.5, 562.1, 439.4)Frame 18: vase at (204.5, 83.4, 402.9, 397.4)Frame 19: person at (156.9, 83.8, 441.7, 551.2)Frame 24: person at (166.9, 85.7, 637.7, 546.1)Frame 32: person at (267.2, 139.9, 569.8, 553.1)Frame 36: car at (159.4, 235.8, 603.2, 500.9)Frame 38: car at (206.2, 193.8, 526.9, 505.6)Frame 45: truck at (189.5, 255.7, 251.9, 321.5)Frame 48: car at (259.1, 135.0, 336.2, 196.0)Frame 50: car at (18.6, 113.7, 441.9, 370.4)Frame 51: car at (0.7, 84.5, 160.8, 530.6)Frame 53: car at (216.6, 279.3, 393.8, 383.3)Frame 54: car at (298.8, 127.6, 466.3, 335.5)Frame 55: car at (0.5, 83.8, 273.8, 366.9)\n",
            "\n",
            "[0.00 - 4.20]  There are many routes one can take in life.[4.20 - 7.64]  Sometimes we take a beautiful[7.64 - 10.92]  but measured journey.[10.92 - 14.92]  But when life is full of choices...[14.92 - 21.92]  sometimes you just let the heart take over.[30.00 - 45.92]  When exhilaration drives you to aim higher.[45.92 - 51.36]  When emotion overtakes all else.[51.36 - 52.88]  KODA SLAVIA 1.5 TSI\n",
            "\n",
            "\n",
            "Video Description: Please generate a detailed textual summary of the video based on Detected Objects and Transcribed Audio in the following order:Frame 1: car at (0.5, 83.9, 85.7, 84.6, 113.7)Frame 2: person at (1.0, 2.0)Frame 3: vase at (2.1)Frame 34: car (3.2)Frame 35: person (4)Frame 36: car[0.00 - 4.64]  Sometimes we take a beautiful[7.20 - 7.20]  There are many routes one can take in life.[4.64 - 10.92]  But when life is full of choices...[14.92 - 14.92)  sometimes you just let the heart take over.[30.00]  When you have to make a choice...[30.30 - 30.92])Frame 37: car(3.1); car (4.5)Frame 38: person(5.0); car(6.0))Frame 39: vehicle at (6.5); car at(443.8, 553.8)Frame 40: vehicle (5.5 - 6.9)  When life is a rollercoaster ride...[6.9 - 8.88]  KODA SLAVIA 1.5 TSI [8.88 - 9.88)Frame 41: vehicle(9.88)]Frame 42: vehicle[10.88])Frame 43: person[11.36 - 52.36] When emotion overtakes all else.[51.2 - 51.2]  but measured journey.[10.36)Frame 44: carat (11.6)Frame 45: truck at (189.2, 196.3, 192.3)Frame 46: vehicleat (13.1, 135.2); personat (14.1; car at)Frame 47: personat(14.5; personat)Frame 48: carAt (16.0; person at)Frames 1-3: Vehicle at (36.0 - 45.20)Frame 4: car 1: (37.00)Frame 5: (38.20, 45.92, 51.92; 51.36, 52.92)]  When exhilaration drives you to aim higher.[45.92- 21.9]Frame 6: (39.00, 41.36), (42.92), (43.0), (44.0)()Frame 7: (40.5), (45.5))Frame 8: (41.5)]Frame 9: (46.5]Frame 10: (48.2), (49.2))Frame 11: (50.1), (51.1))Frame 12: (52.2)()(53.6))Frame 13: (53.1)() (54.1)-(54.2)-(55.2-56.1)+(56.2)+(57.2-)(58.6)+(59.6)-(60.6+)(61.6-)(62.6%)(63.4)(64.4))(65.4)-(66.4)+(67.4+)(68.4-)(69.4-70.4)(72.4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "input = \"\"\"\n",
        "\n",
        "\"\"\" + str()"
      ],
      "metadata": {
        "id": "4syCb8oI-CEX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "from transformers import BartTokenizer, BartForConditionalGeneration\n",
        "import torch\n",
        "\n",
        "# Load BART model and tokenizer\n",
        "tokenizer = BartTokenizer.from_pretrained('facebook/bart-large')\n",
        "bart_model = BartForConditionalGeneration.from_pretrained('facebook/bart-large')\n",
        "\n",
        "# Detect if GPU is available, otherwise use CPU\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "bart_model = bart_model.to(device)\n",
        "\n",
        "def clean_input(input_text):\n",
        "    # Remove unwanted characters or symbols\n",
        "    allowed_chars = ''.join(chr(i) for i in range(32, 127))  # ASCII characters from space to tilde (~)\n",
        "    cleaned_text = ''.join(c for c in input_text if c in allowed_chars)\n",
        "    return cleaned_text\n",
        "\n",
        "def generate_description(detections_summary):\n",
        "    # Combine detections summary and transcribed text\n",
        "    description_input = (f\"\"\"In this frame number are given that are extracted from the video and its detections are given and then bounding boxes are given based on detections. Predict what might be happening in the video.{detections_summary}\"\"\")\n",
        "\n",
        "    # Debug: Print the final input to check formatting\n",
        "    print(\"Description Input:\\n\", description_input)\n",
        "\n",
        "    # Tokenize and generate description\n",
        "    inputs = tokenizer(description_input, return_tensors=\"pt\", max_length=1024, truncation=True).to(device)\n",
        "    summary_ids = bart_model.generate(inputs[\"input_ids\"], max_length=700, min_length=100, length_penalty=2.0, num_beams=4)\n",
        "    description = tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
        "\n",
        "    return description\n",
        "\n",
        "def process_detections_file(detections_file_path):\n",
        "    # Load detections from JSON file\n",
        "    with open(detections_file_path, 'r') as f:\n",
        "        detections_list = json.load(f)\n",
        "\n",
        "\n",
        "\n",
        "    # Sort the detection list by frame number to ensure sequential order\n",
        "    detections_list = sorted(detections_list, key=lambda x: x['frame'])\n",
        "\n",
        "    # Collect detected objects along with their frame numbers and positions (bounding boxes)\n",
        "    frame_objects = {}\n",
        "    for entry in detections_list:\n",
        "        frame = entry.get('frame')\n",
        "        detections = entry.get('detections', [])\n",
        "        objects_in_frame = []\n",
        "\n",
        "        for detection in detections:\n",
        "            if 'class' in detection and 'bbox' in detection:\n",
        "                class_name = detection['class']\n",
        "                bbox = detection['bbox']\n",
        "                # Convert bounding box to a readable format (e.g., top-left and bottom-right)\n",
        "                if len(bbox) == 4:  # Only process if the bounding box has the correct format\n",
        "                    bbox_str = f\"({bbox[0]:.1f}, {bbox[1]:.1f}, {bbox[2]:.1f}, {bbox[3]:.1f})\"\n",
        "                    objects_in_frame.append(f\"{class_name} at {bbox_str}\")\n",
        "\n",
        "        # Save objects detected in this frame\n",
        "        if objects_in_frame:\n",
        "            frame_objects[frame] = objects_in_frame\n",
        "\n",
        "    # Create a formatted summary including objects and their bounding box positions\n",
        "    detections_summary = \"\"\n",
        "    for frame, objects in frame_objects.items():\n",
        "        objects_str = '; '.join(objects)\n",
        "        detections_summary += f\"Frame {frame}: {objects_str}\\n\"\n",
        "\n",
        "    # Debug: Print formatted summaries to check if they are correct\n",
        "    print(\"Detections Summary:\\n\", detections_summary)\n",
        "\n",
        "    # Generate a single description for the whole video including transcribed audio\n",
        "    description = generate_description(detections_summary)\n",
        "\n",
        "    return description\n",
        "\n",
        "# Example usage\n",
        "detections_file_path = 'detections.json'\n",
        "video_description = process_detections_file(detections_file_path)\n",
        "print(f\"Video Description: {video_description}\")\n"
      ],
      "metadata": {
        "id": "ePgh_pJy-z93"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}