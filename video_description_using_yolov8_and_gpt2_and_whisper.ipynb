{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install opencv-python torchvision torchaudio ultralytics\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "IK8KpL-UIfVP",
        "outputId": "4f46967a-d2bf-4145-dccc-23201e832f6d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: opencv-python in /usr/local/lib/python3.10/dist-packages (4.10.0.84)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (0.19.0+cu121)\n",
            "Requirement already satisfied: torchaudio in /usr/local/lib/python3.10/dist-packages (2.4.0+cu121)\n",
            "Collecting ultralytics\n",
            "  Downloading ultralytics-8.2.91-py3-none-any.whl.metadata (41 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m41.9/41.9 kB\u001b[0m \u001b[31m1.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.21.2 in /usr/local/lib/python3.10/dist-packages (from opencv-python) (1.26.4)\n",
            "Requirement already satisfied: torch==2.4.0 in /usr/local/lib/python3.10/dist-packages (from torchvision) (2.4.0+cu121)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision) (9.4.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch==2.4.0->torchvision) (3.15.4)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch==2.4.0->torchvision) (4.12.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch==2.4.0->torchvision) (1.13.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch==2.4.0->torchvision) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch==2.4.0->torchvision) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch==2.4.0->torchvision) (2024.6.1)\n",
            "Requirement already satisfied: matplotlib>=3.3.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (3.7.1)\n",
            "Requirement already satisfied: pyyaml>=5.3.1 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (6.0.2)\n",
            "Requirement already satisfied: requests>=2.23.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (2.32.3)\n",
            "Requirement already satisfied: scipy>=1.4.1 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (1.13.1)\n",
            "Requirement already satisfied: tqdm>=4.64.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (4.66.5)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from ultralytics) (5.9.5)\n",
            "Requirement already satisfied: py-cpuinfo in /usr/local/lib/python3.10/dist-packages (from ultralytics) (9.0.0)\n",
            "Requirement already satisfied: pandas>=1.1.4 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (2.1.4)\n",
            "Requirement already satisfied: seaborn>=0.11.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (0.13.1)\n",
            "Collecting ultralytics-thop>=2.0.0 (from ultralytics)\n",
            "  Downloading ultralytics_thop-2.0.6-py3-none-any.whl.metadata (9.1 kB)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.0->ultralytics) (1.3.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.0->ultralytics) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.0->ultralytics) (4.53.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.0->ultralytics) (1.4.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.0->ultralytics) (24.1)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.0->ultralytics) (3.1.4)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.0->ultralytics) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.1.4->ultralytics) (2024.1)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.1.4->ultralytics) (2024.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.23.0->ultralytics) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.23.0->ultralytics) (3.8)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.23.0->ultralytics) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.23.0->ultralytics) (2024.8.30)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib>=3.3.0->ultralytics) (1.16.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch==2.4.0->torchvision) (2.1.5)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch==2.4.0->torchvision) (1.3.0)\n",
            "Downloading ultralytics-8.2.91-py3-none-any.whl (871 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m871.8/871.8 kB\u001b[0m \u001b[31m15.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading ultralytics_thop-2.0.6-py3-none-any.whl (26 kB)\n",
            "Installing collected packages: ultralytics-thop, ultralytics\n",
            "Successfully installed ultralytics-8.2.91 ultralytics-thop-2.0.6\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers\n"
      ],
      "metadata": {
        "collapsed": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fHhozsepQaoQ",
        "outputId": "d019dd9b-d7bb-40b0-aa31-b95f508c4d0b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.44.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.15.4)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.24.6)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.5.15)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.4)\n",
            "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.19.1)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.5)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (2024.6.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.8)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2024.8.30)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "import torch\n",
        "from transformers import BartTokenizer, BartForConditionalGeneration\n",
        "import json\n",
        "from PIL import Image\n",
        "import io\n",
        "\n",
        "# Load YOLOv8 medium model\n",
        "from ultralytics import YOLO\n",
        "yolo_model = YOLO('yolov8m.pt')\n",
        "\n",
        "# Define class names (this list should match the class IDs used by your YOLO model)\n",
        "class_names = [\n",
        "    'person', 'bicycle', 'car', 'motorcycle', 'airplane', 'bus', 'train', 'truck', 'boat', 'traffic light',\n",
        "    'fire hydrant', 'stop sign', 'parking meter', 'bench', 'bird', 'cat', 'dog', 'horse', 'sheep', 'cow',\n",
        "    'elephant', 'bear', 'zebra', 'giraffe', 'backpack', 'umbrella', 'handbag', 'tie', 'suitcase', 'frisbee',\n",
        "    'skis', 'snowboard', 'sports ball', 'kite', 'baseball bat', 'baseball glove', 'skateboard', 'surfboard',\n",
        "    'tennis racket', 'bottle', 'wine glass', 'cup', 'fork', 'knife', 'spoon', 'bowl', 'banana', 'apple',\n",
        "    'sandwich', 'orange', 'broccoli', 'carrot', 'hot dog', 'pizza', 'donut', 'cake', 'chair', 'couch',\n",
        "    'potted plant', 'bed', 'dining table', 'toilet', 'tv', 'laptop', 'mouse', 'remote', 'keyboard', 'cell phone',\n",
        "    'microwave', 'oven', 'toaster', 'sink', 'refrigerator', 'book', 'clock', 'vase', 'scissors', 'teddy bear',\n",
        "    'hair drier', 'toothbrush'\n",
        "]\n",
        "\n",
        "# Load BART model and tokenizer\n",
        "tokenizer = BartTokenizer.from_pretrained('facebook/bart-large')\n",
        "bart_model = BartForConditionalGeneration.from_pretrained('facebook/bart-large')\n",
        "\n",
        "def generate_description(detections, image):\n",
        "    # Convert image to a format suitable for BART (e.g., base64 or text description)\n",
        "    buffered = io.BytesIO()\n",
        "    image.save(buffered, format=\"JPEG\")\n",
        "    image_str = buffered.getvalue()\n",
        "\n",
        "    # Create a description input for BART\n",
        "    description_input = f\"Frame image data: {image_str}\\nDetections: {json.dumps(detections)}\"\n",
        "\n",
        "    # Tokenize and generate description\n",
        "    inputs = tokenizer(description_input, return_tensors=\"pt\", max_length=1024, truncation=True)\n",
        "    summary_ids = bart_model.generate(inputs[\"input_ids\"], max_length=150, min_length=30, length_penalty=2.0, num_beams=4)\n",
        "    description = tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
        "\n",
        "    return description\n",
        "\n",
        "# Open video file\n",
        "video_path = '/content/The Performance-oriented ŠKODA SLAVIA 1.5 L TSI - A Class of its Own (1).mp4'\n",
        "cap = cv2.VideoCapture(video_path)\n",
        "\n",
        "# Get video properties\n",
        "fps = cap.get(cv2.CAP_PROP_FPS)\n",
        "frame_interval = int(fps)  # Extract 1 frame per second\n",
        "\n",
        "print(f\"Video FPS: {fps}\")\n",
        "print(f\"Frame Interval: {frame_interval}\")\n",
        "\n",
        "# Lists to store frames and detections\n",
        "frame_list = []\n",
        "detections_list = []\n",
        "\n",
        "# Initialize sequential frame counter\n",
        "sequential_frame_count = 1\n",
        "\n",
        "frame_count = 0\n",
        "while cap.isOpened():\n",
        "    ret, frame = cap.read()\n",
        "    if not ret:\n",
        "        print(\"End of video or error reading frame.\")\n",
        "        break\n",
        "\n",
        "    # Extract 1 frame per second\n",
        "    if frame_count % frame_interval == 0:\n",
        "        frame_list.append(frame)\n",
        "\n",
        "        # Resize the frame to the input size expected by YOLOv8\n",
        "        resized_frame = cv2.resize(frame, (640, 640))\n",
        "\n",
        "        # Perform object detection\n",
        "        results = yolo_model(resized_frame)\n",
        "\n",
        "        # Extract detection data with threshold of 0.7\n",
        "        frame_detections = []\n",
        "        for result in results[0].boxes:\n",
        "            bbox = result.xyxy[0].tolist()\n",
        "            confidence = result.conf[0].item()\n",
        "            class_id = int(result.cls[0].item())\n",
        "            class_name = class_names[class_id] if class_id < len(class_names) else 'unknown'\n",
        "\n",
        "            if confidence >= 0.7:\n",
        "                frame_detections.append({\n",
        "                    'bbox': bbox,\n",
        "                    'confidence': confidence,\n",
        "                    'class': class_name\n",
        "                })\n",
        "\n",
        "        # Convert frame to PIL Image\n",
        "        image = Image.fromarray(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB))\n",
        "\n",
        "        # Generate description\n",
        "        #description = generate_description(frame_detections, image)\n",
        "        print(f\"Frame {sequential_frame_count}\")\n",
        "\n",
        "        # Store detections with sequential frame numbering\n",
        "        detections_list.append({\n",
        "            'frame': sequential_frame_count,\n",
        "            'detections': frame_detections,\n",
        "        })\n",
        "\n",
        "        # Increment the sequential frame counter\n",
        "        sequential_frame_count += 1\n",
        "\n",
        "    frame_count += 1\n",
        "\n",
        "cap.release()\n",
        "\n",
        "# Save detections to JSON file\n",
        "with open('detections.json', 'w') as f:\n",
        "    json.dump(detections_list, f, indent=4)\n",
        "\n",
        "print(\"Detections saved to detections.json\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x1bKzIiUOssC",
        "outputId": "7bb13b5b-a283-42a1-b7d3-53940483f705"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Video FPS: 25.0\n",
            "Frame Interval: 25\n",
            "\n",
            "0: 640x640 (no detections), 37.3ms\n",
            "Speed: 2.2ms preprocess, 37.3ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 640)\n",
            "Frame 1\n",
            "\n",
            "0: 640x640 1 scissors, 37.4ms\n",
            "Speed: 2.8ms preprocess, 37.4ms inference, 1.9ms postprocess per image at shape (1, 3, 640, 640)\n",
            "Frame 2\n",
            "\n",
            "0: 640x640 1 scissors, 37.4ms\n",
            "Speed: 2.3ms preprocess, 37.4ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 640)\n",
            "Frame 3\n",
            "\n",
            "0: 640x640 1 car, 37.3ms\n",
            "Speed: 3.5ms preprocess, 37.3ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 640)\n",
            "Frame 4\n",
            "\n",
            "0: 640x640 1 car, 37.3ms\n",
            "Speed: 3.5ms preprocess, 37.3ms inference, 1.3ms postprocess per image at shape (1, 3, 640, 640)\n",
            "Frame 5\n",
            "\n",
            "0: 640x640 1 truck, 37.3ms\n",
            "Speed: 3.1ms preprocess, 37.3ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 640)\n",
            "Frame 6\n",
            "\n",
            "0: 640x640 1 person, 1 car, 37.3ms\n",
            "Speed: 2.6ms preprocess, 37.3ms inference, 1.7ms postprocess per image at shape (1, 3, 640, 640)\n",
            "Frame 7\n",
            "\n",
            "0: 640x640 1 person, 1 car, 37.3ms\n",
            "Speed: 2.7ms preprocess, 37.3ms inference, 1.6ms postprocess per image at shape (1, 3, 640, 640)\n",
            "Frame 8\n",
            "\n",
            "0: 640x640 1 person, 2 cars, 37.3ms\n",
            "Speed: 1.7ms preprocess, 37.3ms inference, 1.3ms postprocess per image at shape (1, 3, 640, 640)\n",
            "Frame 9\n",
            "\n",
            "0: 640x640 1 car, 37.3ms\n",
            "Speed: 2.2ms preprocess, 37.3ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 640)\n",
            "Frame 10\n",
            "\n",
            "0: 640x640 1 car, 37.3ms\n",
            "Speed: 2.3ms preprocess, 37.3ms inference, 1.3ms postprocess per image at shape (1, 3, 640, 640)\n",
            "Frame 11\n",
            "\n",
            "0: 640x640 1 car, 37.5ms\n",
            "Speed: 2.4ms preprocess, 37.5ms inference, 2.5ms postprocess per image at shape (1, 3, 640, 640)\n",
            "Frame 12\n",
            "\n",
            "0: 640x640 (no detections), 37.3ms\n",
            "Speed: 2.7ms preprocess, 37.3ms inference, 0.9ms postprocess per image at shape (1, 3, 640, 640)\n",
            "Frame 13\n",
            "\n",
            "0: 640x640 1 train, 1 cell phone, 37.3ms\n",
            "Speed: 2.4ms preprocess, 37.3ms inference, 1.3ms postprocess per image at shape (1, 3, 640, 640)\n",
            "Frame 14\n",
            "\n",
            "0: 640x640 (no detections), 37.4ms\n",
            "Speed: 3.8ms preprocess, 37.4ms inference, 0.8ms postprocess per image at shape (1, 3, 640, 640)\n",
            "Frame 15\n",
            "\n",
            "0: 640x640 2 cars, 37.3ms\n",
            "Speed: 2.1ms preprocess, 37.3ms inference, 1.3ms postprocess per image at shape (1, 3, 640, 640)\n",
            "Frame 16\n",
            "\n",
            "0: 640x640 2 cars, 1 parking meter, 37.6ms\n",
            "Speed: 2.3ms preprocess, 37.6ms inference, 1.3ms postprocess per image at shape (1, 3, 640, 640)\n",
            "Frame 17\n",
            "\n",
            "0: 640x640 1 vase, 37.3ms\n",
            "Speed: 2.2ms preprocess, 37.3ms inference, 2.2ms postprocess per image at shape (1, 3, 640, 640)\n",
            "Frame 18\n",
            "\n",
            "0: 640x640 2 persons, 37.3ms\n",
            "Speed: 2.4ms preprocess, 37.3ms inference, 1.3ms postprocess per image at shape (1, 3, 640, 640)\n",
            "Frame 19\n",
            "\n",
            "0: 640x640 (no detections), 37.3ms\n",
            "Speed: 2.9ms preprocess, 37.3ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
            "Frame 20\n",
            "\n",
            "0: 640x640 1 person, 37.3ms\n",
            "Speed: 3.3ms preprocess, 37.3ms inference, 1.3ms postprocess per image at shape (1, 3, 640, 640)\n",
            "Frame 21\n",
            "\n",
            "0: 640x640 1 clock, 38.2ms\n",
            "Speed: 2.3ms preprocess, 38.2ms inference, 1.3ms postprocess per image at shape (1, 3, 640, 640)\n",
            "Frame 22\n",
            "\n",
            "0: 640x640 1 clock, 37.3ms\n",
            "Speed: 2.5ms preprocess, 37.3ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 640)\n",
            "Frame 23\n",
            "\n",
            "0: 640x640 2 persons, 37.4ms\n",
            "Speed: 2.8ms preprocess, 37.4ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 640)\n",
            "Frame 24\n",
            "\n",
            "0: 640x640 (no detections), 37.3ms\n",
            "Speed: 2.5ms preprocess, 37.3ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
            "Frame 25\n",
            "\n",
            "0: 640x640 1 clock, 37.3ms\n",
            "Speed: 2.4ms preprocess, 37.3ms inference, 1.3ms postprocess per image at shape (1, 3, 640, 640)\n",
            "Frame 26\n",
            "\n",
            "0: 640x640 1 clock, 37.3ms\n",
            "Speed: 2.6ms preprocess, 37.3ms inference, 1.7ms postprocess per image at shape (1, 3, 640, 640)\n",
            "Frame 27\n",
            "\n",
            "0: 640x640 1 cat, 37.3ms\n",
            "Speed: 3.6ms preprocess, 37.3ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 640)\n",
            "Frame 28\n",
            "\n",
            "0: 640x640 1 scissors, 37.3ms\n",
            "Speed: 3.5ms preprocess, 37.3ms inference, 1.3ms postprocess per image at shape (1, 3, 640, 640)\n",
            "Frame 29\n",
            "\n",
            "0: 640x640 (no detections), 37.3ms\n",
            "Speed: 2.4ms preprocess, 37.3ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
            "Frame 30\n",
            "\n",
            "0: 640x640 1 train, 1 cell phone, 37.4ms\n",
            "Speed: 2.5ms preprocess, 37.4ms inference, 1.6ms postprocess per image at shape (1, 3, 640, 640)\n",
            "Frame 31\n",
            "\n",
            "0: 640x640 1 person, 1 car, 1 cell phone, 37.4ms\n",
            "Speed: 2.5ms preprocess, 37.4ms inference, 1.3ms postprocess per image at shape (1, 3, 640, 640)\n",
            "Frame 32\n",
            "\n",
            "0: 640x640 1 bus, 37.3ms\n",
            "Speed: 3.8ms preprocess, 37.3ms inference, 1.3ms postprocess per image at shape (1, 3, 640, 640)\n",
            "Frame 33\n",
            "\n",
            "0: 640x640 1 car, 1 truck, 37.6ms\n",
            "Speed: 2.3ms preprocess, 37.6ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 640)\n",
            "Frame 34\n",
            "\n",
            "0: 640x640 1 truck, 1 parking meter, 37.5ms\n",
            "Speed: 2.3ms preprocess, 37.5ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 640)\n",
            "Frame 35\n",
            "\n",
            "0: 640x640 1 car, 37.3ms\n",
            "Speed: 2.6ms preprocess, 37.3ms inference, 1.3ms postprocess per image at shape (1, 3, 640, 640)\n",
            "Frame 36\n",
            "\n",
            "0: 640x640 1 car, 37.3ms\n",
            "Speed: 2.2ms preprocess, 37.3ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 640)\n",
            "Frame 37\n",
            "\n",
            "0: 640x640 1 car, 37.3ms\n",
            "Speed: 2.3ms preprocess, 37.3ms inference, 1.3ms postprocess per image at shape (1, 3, 640, 640)\n",
            "Frame 38\n",
            "\n",
            "0: 640x640 1 car, 1 truck, 37.3ms\n",
            "Speed: 2.3ms preprocess, 37.3ms inference, 1.7ms postprocess per image at shape (1, 3, 640, 640)\n",
            "Frame 39\n",
            "\n",
            "0: 640x640 (no detections), 37.4ms\n",
            "Speed: 2.6ms preprocess, 37.4ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
            "Frame 40\n",
            "\n",
            "0: 640x640 1 airplane, 37.3ms\n",
            "Speed: 3.1ms preprocess, 37.3ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 640)\n",
            "Frame 41\n",
            "\n",
            "0: 640x640 (no detections), 37.5ms\n",
            "Speed: 2.1ms preprocess, 37.5ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
            "Frame 42\n",
            "\n",
            "0: 640x640 1 clock, 37.3ms\n",
            "Speed: 2.2ms preprocess, 37.3ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 640)\n",
            "Frame 43\n",
            "\n",
            "0: 640x640 1 clock, 37.3ms\n",
            "Speed: 2.9ms preprocess, 37.3ms inference, 1.3ms postprocess per image at shape (1, 3, 640, 640)\n",
            "Frame 44\n",
            "\n",
            "0: 640x640 1 truck, 37.3ms\n",
            "Speed: 1.9ms preprocess, 37.3ms inference, 1.3ms postprocess per image at shape (1, 3, 640, 640)\n",
            "Frame 45\n",
            "\n",
            "0: 640x640 1 car, 37.6ms\n",
            "Speed: 2.2ms preprocess, 37.6ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 640)\n",
            "Frame 46\n",
            "\n",
            "0: 640x640 (no detections), 37.3ms\n",
            "Speed: 3.6ms preprocess, 37.3ms inference, 0.8ms postprocess per image at shape (1, 3, 640, 640)\n",
            "Frame 47\n",
            "\n",
            "0: 640x640 1 car, 37.3ms\n",
            "Speed: 2.3ms preprocess, 37.3ms inference, 1.6ms postprocess per image at shape (1, 3, 640, 640)\n",
            "Frame 48\n",
            "\n",
            "0: 640x640 (no detections), 37.3ms\n",
            "Speed: 3.2ms preprocess, 37.3ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
            "Frame 49\n",
            "\n",
            "0: 640x640 1 car, 37.3ms\n",
            "Speed: 2.6ms preprocess, 37.3ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 640)\n",
            "Frame 50\n",
            "\n",
            "0: 640x640 1 car, 37.3ms\n",
            "Speed: 2.7ms preprocess, 37.3ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 640)\n",
            "Frame 51\n",
            "\n",
            "0: 640x640 (no detections), 37.3ms\n",
            "Speed: 2.3ms preprocess, 37.3ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
            "Frame 52\n",
            "\n",
            "0: 640x640 1 car, 37.3ms\n",
            "Speed: 3.4ms preprocess, 37.3ms inference, 1.3ms postprocess per image at shape (1, 3, 640, 640)\n",
            "Frame 53\n",
            "\n",
            "0: 640x640 1 car, 37.3ms\n",
            "Speed: 3.4ms preprocess, 37.3ms inference, 1.3ms postprocess per image at shape (1, 3, 640, 640)\n",
            "Frame 54\n",
            "\n",
            "0: 640x640 1 car, 37.4ms\n",
            "Speed: 2.7ms preprocess, 37.4ms inference, 1.8ms postprocess per image at shape (1, 3, 640, 640)\n",
            "Frame 55\n",
            "\n",
            "0: 640x640 (no detections), 37.3ms\n",
            "Speed: 2.5ms preprocess, 37.3ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
            "Frame 56\n",
            "\n",
            "0: 640x640 (no detections), 37.4ms\n",
            "Speed: 2.4ms preprocess, 37.4ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
            "Frame 57\n",
            "\n",
            "0: 640x640 (no detections), 37.5ms\n",
            "Speed: 2.2ms preprocess, 37.5ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
            "Frame 58\n",
            "\n",
            "0: 640x640 1 clock, 37.3ms\n",
            "Speed: 3.0ms preprocess, 37.3ms inference, 1.3ms postprocess per image at shape (1, 3, 640, 640)\n",
            "Frame 59\n",
            "\n",
            "0: 640x640 (no detections), 37.3ms\n",
            "Speed: 3.5ms preprocess, 37.3ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
            "Frame 60\n",
            "\n",
            "0: 640x640 (no detections), 37.4ms\n",
            "Speed: 2.3ms preprocess, 37.4ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
            "Frame 61\n",
            "End of video or error reading frame.\n",
            "Detections saved to detections.json\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -U openai-whisper"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "5jonqegARDLk",
        "outputId": "e1c53c7d-67b7-4ed9-b518-2cbe023f2f05"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting openai-whisper\n",
            "  Downloading openai-whisper-20231117.tar.gz (798 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/798.6 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m266.2/798.6 kB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m \u001b[32m788.5/798.6 kB\u001b[0m \u001b[31m14.7 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m798.6/798.6 kB\u001b[0m \u001b[31m10.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting triton<3,>=2.0.0 (from openai-whisper)\n",
            "  Downloading triton-2.3.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.4 kB)\n",
            "Requirement already satisfied: numba in /usr/local/lib/python3.10/dist-packages (from openai-whisper) (0.60.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from openai-whisper) (1.26.4)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from openai-whisper) (2.4.0+cu121)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from openai-whisper) (4.66.5)\n",
            "Requirement already satisfied: more-itertools in /usr/local/lib/python3.10/dist-packages (from openai-whisper) (10.3.0)\n",
            "Collecting tiktoken (from openai-whisper)\n",
            "  Downloading tiktoken-0.7.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.6 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from triton<3,>=2.0.0->openai-whisper) (3.15.4)\n",
            "Requirement already satisfied: llvmlite<0.44,>=0.43.0dev0 in /usr/local/lib/python3.10/dist-packages (from numba->openai-whisper) (0.43.0)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.10/dist-packages (from tiktoken->openai-whisper) (2024.5.15)\n",
            "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.10/dist-packages (from tiktoken->openai-whisper) (2.32.3)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch->openai-whisper) (4.12.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch->openai-whisper) (1.13.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->openai-whisper) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->openai-whisper) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch->openai-whisper) (2024.6.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken->openai-whisper) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken->openai-whisper) (3.8)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken->openai-whisper) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken->openai-whisper) (2024.8.30)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->openai-whisper) (2.1.5)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch->openai-whisper) (1.3.0)\n",
            "Downloading triton-2.3.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (168.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m168.1/168.1 MB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tiktoken-0.7.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m43.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: openai-whisper\n",
            "  Building wheel for openai-whisper (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for openai-whisper: filename=openai_whisper-20231117-py3-none-any.whl size=801360 sha256=d1f70006406e18af96b18ed91a6c90b55112e42887470326f7bcd42552de614c\n",
            "  Stored in directory: /root/.cache/pip/wheels/d0/85/e1/9361b4cbea7dd4b7f6702fa4c3afc94877952eeb2b62f45f56\n",
            "Successfully built openai-whisper\n",
            "Installing collected packages: triton, tiktoken, openai-whisper\n",
            "Successfully installed openai-whisper-20231117 tiktoken-0.7.0 triton-2.3.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import whisper\n",
        "\n",
        "def transcribe_audio(audio_path, output_file):\n",
        "    model = whisper.load_model(\"medium\")\n",
        "    result = model.transcribe(audio_path, language=\"en\", temperature=0.6, verbose=True)\n",
        "\n",
        "    with open(output_file, 'w') as f:\n",
        "        for segment in result['segments']:\n",
        "            start_time = segment['start']\n",
        "            end_time = segment['end']\n",
        "            text = segment['text']\n",
        "            f.write(f\"[{start_time:.2f} - {end_time:.2f}] {text}\\n\")\n",
        "\n",
        "# Example usage\n",
        "audio_path = \"/content/audio file.m4a\"\n",
        "output_file = \"/content/transcription_with_timestamps.txt\"\n",
        "transcribe_audio(audio_path, output_file)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IiVNXsHLRB6c",
        "outputId": "e83533e1-51d3-47ee-d326-da2dd1793a10",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|█████████████████████████████████████| 1.42G/1.42G [00:20<00:00, 76.3MiB/s]\n",
            "/usr/local/lib/python3.10/dist-packages/whisper/__init__.py:146: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  checkpoint = torch.load(fp, map_location=device)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[00:00.000 --> 00:04.200]  There are many routes one can take in life.\n",
            "[00:04.200 --> 00:07.640]  Sometimes we take a beautiful\n",
            "[00:07.640 --> 00:10.920]  but measured journey.\n",
            "[00:10.920 --> 00:14.920]  But when life is full of choices...\n",
            "[00:14.920 --> 00:21.920]  sometimes you just let the heart take over.\n",
            "[00:30.000 --> 00:45.920]  When exhilaration drives you to aim higher.\n",
            "[00:45.920 --> 00:51.360]  When emotion overtakes all else.\n",
            "[00:51.360 --> 00:52.880]  ŠKODA SLAVIA 1.5 TSI\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
        "import torch\n",
        "\n",
        "# Load GPT-2 model and tokenizer\n",
        "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
        "gpt2_model = GPT2LMHeadModel.from_pretrained('gpt2')\n",
        "\n",
        "# Detect if GPU is available, otherwise use CPU\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "gpt2_model = gpt2_model.to(device)\n",
        "\n",
        "def clean_input(input_text):\n",
        "    # Remove unwanted characters or symbols\n",
        "    allowed_chars = ''.join(chr(i) for i in range(32, 127))  # ASCII characters from space to tilde (~)\n",
        "    cleaned_text = ''.join(c for c in input_text if c in allowed_chars)\n",
        "    return cleaned_text\n",
        "\n",
        "def generate_description(detections_summary, transcribed_text):\n",
        "    # Combine detections summary and transcribed text\n",
        "    # description_input = (\n",
        "    #    f\"\"\"You are generating a detailed description of a commercial TV ad based on visual object detections and transcribed text.\n",
        "\n",
        "    #     1. **Visual Detections**: The following objects were detected in the video with their positions and sizes described by four values: the x-coordinate and y-coordinate of the center, followed by the width and height of each object. Use this data to understand where the objects appear in the video.\n",
        "\n",
        "    #     {clean_input(detections_summary)}\n",
        "\n",
        "    #     2. **Transcribed Text**: This is the audio transcription from the video, with corresponding timestamps. Use this to understand the context and actions happening in the video.\n",
        "\n",
        "    #     {clean_input(transcribed_text)}\n",
        "\n",
        "    #     Based on this information, generate a rich, descriptive explanation of what is happening in the video, keeping in mind that it is a commercial TV ad.\"\"\"\n",
        "    #These are detections of objects in the video of commercial tv ad with their positions and sizes described by four values: the x-coordinate and y-coordinate of the center, followed by the width and height of each object. Use this data to understand where the objects appear in the video:\n",
        "    # )\n",
        "    description_input = (\n",
        "        f\"\"\"detections of the objects in the commercial tv ad video are given along with there coordinates\"\"\"\n",
        "\n",
        "        +str(clean_input(detections_summary))\n",
        "\n",
        "        +\"\"\"Additionally this is the transcript of the video with the timestamps:\"\"\"\n",
        "\n",
        "        +str(clean_input(transcribed_text))\n",
        "\n",
        "        +\"\"\"###Baesd on this information, generate a coherent and detailed description of the video content, emphasizing the visual elements and how they relate to the spoken content.\"\"\"\n",
        "     )\n",
        "\n",
        "\n",
        "    # Debug: Print the final input to check formatting\n",
        "    print(\"Description Input:\\n\", description_input)\n",
        "\n",
        "    # Tokenize and generate description\n",
        "    inputs = tokenizer(description_input, return_tensors=\"pt\", max_length=1024, truncation=True).to(device)\n",
        "    outputs = gpt2_model.generate(\n",
        "        inputs[\"input_ids\"],\n",
        "        max_new_tokens=200,  # Only limits the number of tokens generated, not affecting input\n",
        "        min_length=150,\n",
        "        no_repeat_ngram_size=2,\n",
        "        early_stopping=True,\n",
        "        temperature=0.8,\n",
        "        do_sample = True,\n",
        "        top_k=50,\n",
        "        top_p=0.85\n",
        "    )\n",
        "    description = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "    return description\n",
        "\n",
        "def process_detections_file(detections_file_path, transcribed_audio_file_path):\n",
        "    # Load detections from JSON file\n",
        "    with open(detections_file_path, 'r') as f:\n",
        "        detections_list = json.load(f)\n",
        "\n",
        "    # Load transcribed audio text from TXT file\n",
        "    with open(transcribed_audio_file_path, 'r') as f:\n",
        "        transcribed_text = f.read()\n",
        "\n",
        "    # Sort the detection list by frame number to ensure sequential order\n",
        "    detections_list = sorted(detections_list, key=lambda x: x['frame'])\n",
        "\n",
        "    # Collect detected objects along with their frame numbers and positions (bounding boxes)\n",
        "    frame_objects = {}\n",
        "    for entry in detections_list:\n",
        "        frame = entry.get('frame')\n",
        "        detections = entry.get('detections', [])\n",
        "        objects_in_frame = []\n",
        "\n",
        "        for detection in detections:\n",
        "            if 'class' in detection and 'bbox' in detection:\n",
        "                class_name = detection['class']\n",
        "                bbox = detection['bbox']\n",
        "                # Convert bounding box to a readable format (e.g., top-left and bottom-right)\n",
        "                if len(bbox) == 4:  # Only process if the bounding box has the correct format\n",
        "                    bbox_str = f\"({bbox[0]:.1f}, {bbox[1]:.1f}, {bbox[2]:.1f}, {bbox[3]:.1f})\"\n",
        "                    objects_in_frame.append(f\"{class_name} at {bbox_str}\")\n",
        "\n",
        "        # Save objects detected in this frame\n",
        "        if objects_in_frame:\n",
        "            frame_objects[frame] = objects_in_frame\n",
        "\n",
        "    # Create a formatted summary including objects and their bounding box positions\n",
        "    detections_summary = \"\"\n",
        "    for frame, objects in frame_objects.items():\n",
        "        objects_str = '; '.join(objects)\n",
        "        detections_summary += f\"Frame {frame}: {objects_str}\\n\"\n",
        "\n",
        "    # Debug: Print formatted summaries to check if they are correct\n",
        "    print(\"Detections Summary:\\n\", detections_summary)\n",
        "    print(\"Transcribed Text:\\n\", transcribed_text)\n",
        "\n",
        "    # Generate a single description for the whole video including transcribed audio\n",
        "    description = generate_description(detections_summary, transcribed_text)\n",
        "\n",
        "    return description\n",
        "\n",
        "# Example usage\n",
        "detections_file_path = 'detections.json'\n",
        "transcribed_audio_file_path = 'transcription_with_timestamps.txt'\n",
        "video_description = process_detections_file(detections_file_path, transcribed_audio_file_path)\n",
        "print(f\"Video Explanation: {video_description}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DWsbcg1WjxeN",
        "outputId": "9d6b65c1-255d-4fa1-aa26-cdc55e20df8c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Detections Summary:\n",
            " Frame 4: car at (36.8, 146.4, 157.4, 267.7)\n",
            "Frame 5: car at (94.7, 150.7, 220.0, 282.7)\n",
            "Frame 7: car at (287.3, 192.9, 455.1, 430.0)\n",
            "Frame 8: car at (304.8, 221.8, 444.2, 418.0); person at (377.3, 219.3, 382.7, 234.5)\n",
            "Frame 9: car at (289.4, 238.6, 423.5, 406.1); car at (169.8, 342.6, 224.5, 405.6)\n",
            "Frame 10: car at (174.7, 327.7, 256.7, 411.2)\n",
            "Frame 11: car at (192.3, 296.1, 337.0, 422.1)\n",
            "Frame 16: car at (55.4, 239.0, 460.2, 479.5); car at (443.8, 334.5, 562.1, 439.4)\n",
            "Frame 18: vase at (204.5, 83.4, 402.9, 397.4)\n",
            "Frame 19: person at (156.9, 83.8, 441.7, 551.2)\n",
            "Frame 24: person at (166.9, 85.7, 637.7, 546.1)\n",
            "Frame 32: person at (267.2, 139.9, 569.8, 553.1)\n",
            "Frame 36: car at (159.4, 235.8, 603.2, 500.9)\n",
            "Frame 38: car at (206.2, 193.8, 526.9, 505.6)\n",
            "Frame 45: truck at (189.5, 255.7, 251.9, 321.5)\n",
            "Frame 48: car at (259.1, 135.0, 336.2, 196.0)\n",
            "Frame 50: car at (18.6, 113.7, 441.9, 370.4)\n",
            "Frame 51: car at (0.7, 84.5, 160.8, 530.6)\n",
            "Frame 53: car at (216.6, 279.3, 393.8, 383.3)\n",
            "Frame 54: car at (298.8, 127.6, 466.3, 335.5)\n",
            "Frame 55: car at (0.5, 83.8, 273.8, 366.9)\n",
            "\n",
            "Transcribed Text:\n",
            " [0.00 - 4.20]  There are many routes one can take in life.\n",
            "[4.20 - 7.64]  Sometimes we take a beautiful\n",
            "[7.64 - 10.92]  but measured journey.\n",
            "[10.92 - 14.92]  But when life is full of choices...\n",
            "[14.92 - 21.92]  sometimes you just let the heart take over.\n",
            "[30.00 - 45.92]  When exhilaration drives you to aim higher.\n",
            "[45.92 - 51.36]  When emotion overtakes all else.\n",
            "[51.36 - 52.88]  ŠKODA SLAVIA 1.5 TSI\n",
            "\n",
            "Description Input:\n",
            " detections of the objects in the commercial tv ad video are given along with there coordinatesFrame 4: car at (36.8, 146.4, 157.4, 267.7)Frame 5: car at (94.7, 150.7, 220.0, 282.7)Frame 7: car at (287.3, 192.9, 455.1, 430.0)Frame 8: car at (304.8, 221.8, 444.2, 418.0); person at (377.3, 219.3, 382.7, 234.5)Frame 9: car at (289.4, 238.6, 423.5, 406.1); car at (169.8, 342.6, 224.5, 405.6)Frame 10: car at (174.7, 327.7, 256.7, 411.2)Frame 11: car at (192.3, 296.1, 337.0, 422.1)Frame 16: car at (55.4, 239.0, 460.2, 479.5); car at (443.8, 334.5, 562.1, 439.4)Frame 18: vase at (204.5, 83.4, 402.9, 397.4)Frame 19: person at (156.9, 83.8, 441.7, 551.2)Frame 24: person at (166.9, 85.7, 637.7, 546.1)Frame 32: person at (267.2, 139.9, 569.8, 553.1)Frame 36: car at (159.4, 235.8, 603.2, 500.9)Frame 38: car at (206.2, 193.8, 526.9, 505.6)Frame 45: truck at (189.5, 255.7, 251.9, 321.5)Frame 48: car at (259.1, 135.0, 336.2, 196.0)Frame 50: car at (18.6, 113.7, 441.9, 370.4)Frame 51: car at (0.7, 84.5, 160.8, 530.6)Frame 53: car at (216.6, 279.3, 393.8, 383.3)Frame 54: car at (298.8, 127.6, 466.3, 335.5)Frame 55: car at (0.5, 83.8, 273.8, 366.9)Additionally this is the transcript of the video with the timestamps:[0.00 - 4.20]  There are many routes one can take in life.[4.20 - 7.64]  Sometimes we take a beautiful[7.64 - 10.92]  but measured journey.[10.92 - 14.92]  But when life is full of choices...[14.92 - 21.92]  sometimes you just let the heart take over.[30.00 - 45.92]  When exhilaration drives you to aim higher.[45.92 - 51.36]  When emotion overtakes all else.[51.36 - 52.88]  KODA SLAVIA 1.5 TSI\n",
            "Video Explanation: detections of the objects in the commercial tv ad video are given along with there coordinatesFrame 4: car at (36.8, 146.4, 157.4, 267.7)Frame 5: car at (94.7, 150.7, 220.0, 282.7)Frame 7: car at (287.3, 192.9, 455.1, 430.0)Frame 8: car at (304.8, 221.8, 444.2, 418.0); person at (377.3, 219.3, 382.7, 234.5)Frame 9: car at (289.4, 238.6, 423.5, 406.1); car at (169.8, 342.6, 224.5, 405.6)Frame 10: car at (174.7, 327.7, 256.7, 411.2)Frame 11: car at (192.3, 296.1, 337.0, 422.1)Frame 16: car at (55.4, 239.0, 460.2, 479.5); car at (443.8, 334.5, 562.1, 439.4)Frame 18: vase at (204.5, 83.4, 402.9, 397.4)Frame 19: person at (156.9, 83.8, 441.7, 551.2)Frame 24: person at (166.9, 85.7, 637.7, 546.1)Frame 32: person at (267.2, 139.9, 569.8, 553.1)Frame 36: car at (159.4, 235.8, 603.2, 500.9)Frame 38: car at (206.2, 193.8, 526.9, 505.6)Frame 45: truck at (189.5, 255.7, 251.9, 321.5)Frame 48: car at (259.1, 135.0, 336.2, 196.0)Frame 50: car at (18.6, 113.7, 441.9, 370.4)Frame 51: car at (0.7, 84.5, 160.8, 530.6)Frame 53: car at (216.6, 279.3, 393.8, 383.3)Frame 54: car at (298.8, 127.6, 466.3, 335.5)Frame 55: car at (0.5, 83.8, 273.8, 366.9)Additionally this is the transcript of the video with the timestamps:[0.00 - 4.20]  There are many routes one can take in life.[4.20 - 7.64]  Sometimes we take a beautiful[7.64 - 10.92]  but measured journey.[10.92 - 14.92]  But when life is full of choices...[14.92 - 21.92]  sometimes you just let the heart take over.[30.00 - 45.92]  When exhilaration drives you to aim higher.[45.92 - 51.36]  When emotion overtakes all else.[51.36 - 52.88]  KODA SLAVIA 1.5 TSI - T2 - R2 T3 - L2 S3 T4 T5 - B1 T6 T7 T8 T9 T10 T11 T12 T13 T14 T15 T16 T17 T18 T19 T20 T21 T22 T23 T24 T25 T26 T27 T28 T29 T30 T31 T32 T33 T34 T35 T36 T37 T38 T39 T40 T41 T42 T43 T44 T45 T46 T47 T48 T49 T50 T51 T52 T53 T54 T55 T56 T57 T58 T59 T60 T61 T62 T63 T64 T65 T66 T67 T68 T69 T70 T71 T72 T73 T74 T75 T76 T77 T78 T79 T80 T81 T82 T83 T84 T85 T86 T87 T88 T89 T90 T91 T92 T93 T94 T95\n"
          ]
        }
      ]
    }
  ]
}